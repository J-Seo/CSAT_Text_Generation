{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "from model import Model\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "from rouge import Rouge\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sym_vocab\n",
    "import ant_vocab\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문 생성 및 문제 생성을 위한 텍스트 파일 생성 \n",
    "def crawl_news(url, raw_txt_name, num_word):\n",
    "    # URL 기반 크롤링, 언어는 영어로 설정\n",
    "    news = Article(url, language = 'en')\n",
    "    news.download()\n",
    "    # HTML 파싱 적용\n",
    "    news.parse()\n",
    "    # 지정된 단어 갯수를 기준으로 textrank를 적용하여 파싱된 텍스트를 요약 (Rule base)\n",
    "    text = summarize(news.text, word_count = num_word) # 800자 기준으로 하기\n",
    "    title = news.title\n",
    "    title = title.lower()\n",
    "    \n",
    "    title = re.sub('\\([^)]*\\)', '', title) # 괄호 안 제거 \n",
    "    \n",
    "    with open(raw_txt_name, 'w', encoding = 'utf-8') as f: # 중첩 형식 사용해보기 \n",
    "        f.write(text)\n",
    "    \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "\n",
    "    return text, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_create_topic(raw_txt_name, prepro_txt_name):\n",
    "    # raw_txt_name = './CSAT_topic.txt'\n",
    "    test_dict = {}\n",
    "\n",
    "    with open(raw_txt_name, 'r', encoding = 'utf-8') as f:\n",
    "    # print(f.readlines()) # 데이터 형식은 리스트, 리스트 안에 문자열로 들어있음.\n",
    "        for idx, text in enumerate(f.readlines()):\n",
    "            # print(idx, text)\n",
    "            text = text.lower() # 소문자로 통일\n",
    "            text = re.sub('[.]', '', text) # 일단 온점 모두 제거\n",
    "            text = re.sub('[,]', ' ,', text) # 반점 하나의 단어로 인식\n",
    "            text = re.sub(\"\\’s\",' \\'s', text) # 소유격도 하나의 단어로 인식\n",
    "            text = re.sub(\"[0-9]\", '#', text) # 숫자를 #로 치환\n",
    "            #text = re.sub(\"'.+'\", '', text) # 작은 따옴표 안 고유명사 제거 \n",
    "            text = re.sub('[“”]', '', text) # 큰 따옴표 제거 \n",
    "            text = re.sub('\\n', '', text) # 줄 바꿈표 제거 \n",
    "            print(idx, text)\n",
    "        \n",
    "            # 사전에 저장하기\n",
    "            test_dict[idx] = text\n",
    "    \n",
    "    with open(prepro_txt_name, 'w', encoding = 'utf-8') as f: # 중첩 형식 사용해보기\n",
    "    \n",
    "        for i in range(len(test_dict)):\n",
    "            test_dict[i] = test_dict[i] + \" \" + \".\" + '\\n'\n",
    "            f.write(test_dict[i])\n",
    "    \n",
    "    return test_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shake_bot_topic(filename, valid_article_path):\n",
    "    with open(\"args.pickle\", \"rb\") as f:\n",
    "        args = pickle.load(f)\n",
    "    \n",
    "    # 재사용 가능 \n",
    "    tf.reset_default_graph()  \n",
    "    \n",
    "    print(\"셰봇이 단어 사전 불러오는 중...\")\n",
    "    word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "    print(\"셰봇이 기본 설정 값 초기화 중...\")\n",
    "    valid_x = build_dataset(\"valid\", valid_article_path, word_dict, article_max_len, summary_max_len, args.toy)\n",
    "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Loading saved model...\")\n",
    "        model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "        print(\"셰봇이 수능 문제 생성 준비 중.. {}...\".format(filename))\n",
    "        for batch_x, _ in batches:\n",
    "            batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "            valid_feed_dict = {\n",
    "                model.batch_size: len(batch_x),\n",
    "                model.X: batch_x,\n",
    "                model.X_len: batch_x_len,\n",
    "            }\n",
    "\n",
    "            prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "            prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "\n",
    "            with open(filename, \"w\") as f: # 중첩 형식 사용해보기 \n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    print(\" \".join(summary), file=f)\n",
    "                    \n",
    "            #with open('./rouge_test/sys_dir/{}.txt'.format(filename), \"w\") as f:\n",
    "                #for line in prediction_output:\n",
    "                    #summary = list()\n",
    "                    #for word in line:\n",
    "                        #if word == \"</s>\":\n",
    "                            #break\n",
    "                        #if word not in summary:\n",
    "                            #summary.append(word)\n",
    "                    #print(\" \".join(summary), file=f)\n",
    "\n",
    "        print('셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. {}...'.format(filename))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_rouge_test(summy, label):\n",
    "    rouge_dict = {}\n",
    "    rouge_list = []\n",
    "    rouge = Rouge()\n",
    "        \n",
    "    with open(summy, 'r', encoding = 'utf-8') as f:\n",
    "            \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            scores = rouge.get_scores(line, label[idx])\n",
    "            \n",
    "            p_score = scores[0][\"rouge-1\"][\"p\"] \n",
    "            r_score = scores[0][\"rouge-1\"][\"r\"] \n",
    "            \n",
    "\n",
    "            r_idx_score = (idx, r_score)            \n",
    "            \n",
    "            rouge_list.append(r_idx_score)\n",
    "            # print(rouge_list)\n",
    "            \n",
    "            rouge_dict[idx] = line\n",
    "            \n",
    "        sort_rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[-1], reverse=True)\n",
    "        sort_rouge_list = sort_rouge_list[:4]\n",
    "        \n",
    "        question_list = []\n",
    "        \n",
    "        for ix in sort_rouge_list:\n",
    "            \n",
    "            question_list.append(rouge_dict[ix[0]])\n",
    "        \n",
    "   \n",
    "    return question_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_rouge_test_correct(summy, label):\n",
    "    rouge_dict = {}\n",
    "    rouge_list = []\n",
    "    rouge = Rouge()\n",
    "        \n",
    "    with open(summy, 'r', encoding = 'utf-8') as f:\n",
    "            \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            scores = rouge.get_scores(line, label[idx])\n",
    "            \n",
    "            p_score = scores[0][\"rouge-1\"][\"p\"] \n",
    "            r_score = scores[0][\"rouge-1\"][\"r\"] \n",
    "            \n",
    "            if r_score > 0:\n",
    "            \n",
    "                r_idx_score = (idx, r_score)            \n",
    "                \n",
    "                rouge_list.append(r_idx_score)\n",
    "                # print(rouge_list)\n",
    "            \n",
    "                rouge_dict[idx] = line\n",
    "            \n",
    "            \n",
    "        rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[1], reverse=False)\n",
    "        \n",
    "        rouge_list = rouge_list[:5]\n",
    "        \n",
    "        sort_rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[0], reverse=False)\n",
    "                \n",
    "        question_list = []\n",
    "    \n",
    "        for ix in sort_rouge_list:\n",
    "            \n",
    "            question_list.append(rouge_dict[ix[0]])\n",
    "          \n",
    "    return question_list, sort_rouge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unk(content, unk_sentence):\n",
    "    \n",
    "    container = []\n",
    "    \n",
    "    text = open(content, 'r', encoding = 'utf-8').read()\n",
    "    \n",
    "    tagged_list = pos_tag(word_tokenize(text))\n",
    "    \n",
    "    \n",
    "    nnp_list = [t[0] for t in tagged_list if (t[1] == \"NNP\") or (t[1] == \"NN\")]\n",
    "    \n",
    "    fd_names = FreqDist(nnp_list)\n",
    "    \n",
    "    freq = fd_names.most_common(2)\n",
    "    \n",
    "    #for tagged in tagged_list:\n",
    "        \n",
    "        #if tagged[1] == 'NN' or 'NNP':\n",
    "            \n",
    "            #container.append(tagged[0]) # 단어만 추출\n",
    "    \n",
    "    \n",
    "    \n",
    "    replace_sentence = re.sub('< unk >', freq[0][0], unk_sentence)\n",
    "    \n",
    "    replace_sentence = re.sub('# nd', 'Several', replace_sentence)\n",
    "    \n",
    "    replace_sentence = re.sub('\\n', '', replace_sentence)\n",
    "    \n",
    "    return replace_sentence\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wrong_question(replace_sentence): # 딱 하나만 들어가야 함.\n",
    "    \n",
    "    \n",
    "    token = word_tokenize(replace_sentence)\n",
    "    \n",
    "    tagged_list = pos_tag(token)\n",
    "    \n",
    "    va_list = [t[0] for t in tagged_list if (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\")\n",
    "               or (t[1] == \"RB\") or (t[1] == \"RBS\") or (t[1] == \"RBR\") or (t[1] == \"VB\") or (t[1] == \"VBD\")]\n",
    "    \n",
    "    \n",
    "    for va in va_list:\n",
    "        \n",
    "        para = ant_vocab.crawling_dict(va)\n",
    "        \n",
    "        if len(para) > 0:\n",
    "            \n",
    "            token[token.index(va)] = para  \n",
    "            \n",
    "        \n",
    "    replace_sentence = ' '.join(token)\n",
    "    \n",
    "    replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "    \n",
    "    return replace_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_order_sentence(test_dict):\n",
    "    \n",
    "    # 제시 문장 \n",
    "    main_set = '(A)' + ' ' + test_dict[0]\n",
    "    \n",
    "    # 몇개의 문장씩 나눌 것인지 결정\n",
    "    \n",
    "    divide_num = (int(len(test_dict)-1)) // 3\n",
    "    \n",
    "    print(divide_num)\n",
    "    \n",
    "    remain_num = (int(len(test_dict)-1)) % 3\n",
    "    \n",
    "    print(remain_num)\n",
    "    \n",
    "    if divide_num >= remain_num * 2:\n",
    "        \n",
    "        divide_num = int(divide_num - 1)\n",
    "        \n",
    "        remain_num = int(remain_num + 2)\n",
    "    \n",
    "    \n",
    "    # 랜덤하게 배정하기 위한 리스트 형성\n",
    "    \n",
    "    assign_list = [int(divide_num), int(divide_num), int(remain_num)]\n",
    "    random.shuffle(assign_list)\n",
    "    \n",
    "    # 배정받은 수만큼 문장 나누기\n",
    "    \n",
    "    first_set = list()\n",
    "    second_set = list()\n",
    "    third_set = list()\n",
    "    \n",
    "    for n in range(1, assign_list[0] + 1):\n",
    "        first_set.append(test_dict[n].capitalize())\n",
    "        \n",
    "    \n",
    "    for n in range(assign_list[0] + 1, assign_list[0] + assign_list[1] + 1):\n",
    "        second_set.append(test_dict[n].capitalize())\n",
    "        \n",
    "    \n",
    "    for n in range(assign_list[0] + assign_list[1] + 1, assign_list[0] + assign_list[1] + assign_list[2] + 1):\n",
    "        third_set.append(test_dict[n].capitalize())\n",
    "    \n",
    "    \n",
    "    return main_set, first_set, second_set, third_set # 아직 배열 되어 있지 않은 상황. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ms warren offered her transition plan two weeks after she released a detailed proposal to finance a medicare for all system , at a cost of $### trillion in additional federal spending over a decade\n",
      "1 ms warren put forward that plan to rebut incessant questions about whether she would raise taxes on the middle class in order to fund a sweeping new government health insurance program\n",
      "2 to fund a full-scale medicare for all program , ms warren would rely on big tax increases on businesses and wealthy americans , and she said that she would not increase taxes on middle-class families by one penny her plan was met with harsh criticism from mr biden , whose campaign said it was unrealistic and required mathematical gymnastics\n",
      "3 her transition plan did not come with its own detailed financing proposal , but it would cost the federal government less than an eventual medicare for all system , over all , and would be funded using some mix of the revenue sources she has already identified , according to her plan\n",
      "4 when she released her financing proposal , ms warren said she would release details at a later date regarding the transition to medicare for all\n",
      "5 with her announcement on friday , less than a week before the next democratic debate , she is not only fleshing out her vision for the future of america 's health care system under a warren administration , but strongly suggesting it will be among her top priorities\n",
      "6 ms warren 's plan also spells out a long list of administrative actions she would take to change the health care system , even if democrats do not retake the senate\n",
      "7 she would roll back many trump administration regulations that have weakened the affordable care act and shore up benefits in the existing medicare program\n",
      "8 she would also use executive authority to allow new generic versions of expensive drugs , such as insulins taken by patients with diabetes , treatments for hepatitis c , and the overdose reversal drug naloxone , an effort to lower their prices\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 문제 생성 준비 중.. long_41.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. long_41.txt...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[41 ~ 42] 다음 글을 읽고, 물음에 답하시오.\n",
      "-----------------------------------\n",
      "Ms. Warren offered her transition plan two weeks after she released a detailed proposal to finance a Medicare for all system, at a cost of $20.5 trillion in additional federal spending over a decade. Ms. Warren put forward that plan to rebut incessant questions about whether she would raise taxes on the middle class in order to fund a sweeping new government health insurance program. To fund a full-scale Medicare for all program, Ms. Warren would rely on big tax increases on businesses and wealthy Americans, and she said that she would not increase taxes on middle-class families by “one penny.” Her plan was met with harsh criticism from Mr. Biden, whose campaign said it was “unrealistic” and required “mathematical gymnastics.” Her transition plan did not come with its own detailed financing proposal, but it would cost the federal government less than an eventual Medicare for all system, over all, and would be funded using some mix of the revenue sources she has already identified, according to her plan. When she released her financing proposal, Ms. Warren said she would release details at a later date regarding the transition to Medicare for all. With her announcement on Friday, less than a week before the next Democratic debate, she is not only fleshing out her vision for the future of America’s health care system under a Warren administration, but strongly suggesting it will be among her top priorities. Ms. Warren’s plan also spells out a long list of administrative actions she would take to change the health care system, even if Democrats do not retake the Senate. She would roll back many Trump administration regulations that have weakened the Affordable Care Act and shore up benefits in the existing Medicare program. She would also use executive authority to allow new generic versions of expensive drugs, such as insulins taken by patients with diabetes, treatments for hepatitis C, and the overdose reversal drug Naloxone, an effort to lower their prices.\n",
      "\n",
      "\n",
      "41. 윗글의 제목으로 가장 적절한 것은?\n",
      "① Ms Warren To Release Details On Medicare.\n",
      "② U # S Democratic Plan To Change Health Care System.\n",
      "③ Elizabeth Warren Vows To Expand Health Coverage In First 100 Days.\n",
      "④ Plan To Lower Prices Of Generic Drugs.\n",
      "⑤ Ms Warren Puts Taxes On Middle Class.\n",
      "u는 사전에 없는 단어입니다\n",
      "\n",
      "\n",
      "42. 윗글에 관한 내용으로 적절하지 않은 것은?\n",
      "① Ms warren offers transition plan.\n",
      "② U # s to decrease taxes on plan..\n",
      "③ U # s government plans to fund new medicare plan.\n",
      "④ New u # s health care system outlined.\n",
      "⑤ U # s consumers benefit from medicare reform.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2019/11/15/us/politics/elizabeth-warren-medicare-for-all-100-days.html?action=click&module=Top%20Stories&pgtype=Homepage'\n",
    "num_word = 800\n",
    "raw_txt_name = 'CSAT_41.txt'\n",
    "prepro_txt_name = 'prepro_41.txt'\n",
    "learned_txt_name = 'long_41.txt'\n",
    "num = 1\n",
    "\n",
    "def long_41(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_word, num):\n",
    "    \n",
    "    text, title = crawl_news(url, raw_txt_name, num_word) # url과 raw_txt_name 파일명 입력\n",
    "    \n",
    "    #print(title)\n",
    "    \n",
    "    test_dict = bf_create_topic(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    \n",
    "    shake_bot_topic(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    question_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    #print(question_list)\n",
    "\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        replace_sentence = replace_sentence.title() + \".\"\n",
    "        \n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    #print(question_list)\n",
    "    \n",
    "    title = re.sub('\\n+', '', title)\n",
    "    \n",
    "    title = title.strip()\n",
    "    \n",
    "    title = title.title() + \".\"\n",
    "    \n",
    "    question_list.append(title)\n",
    "    \n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('[41 ~ 42] 다음 글을 읽고, 물음에 답하시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text) # 지문 800자\n",
    "    print('\\n')\n",
    "    \n",
    "    print('41. 윗글의 제목으로 가장 적절한 것은?')\n",
    "    \n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    question_list, sort_rouge_list = for_rouge_test_correct(learned_txt_name, test_dict) # 5개의 선지가 나옴\n",
    "    \n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == num: # 오답 선지 1개 \n",
    "            \n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx != num: # 정답 선지 4개\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            #print(tag)\n",
    "            \n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # print(v_list)\n",
    "            \n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "                \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                    \n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                \n",
    "            \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "    \n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print('42. 윗글에 관한 내용으로 적절하지 않은 것은?')\n",
    "    \n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    long_41(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_word, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a group of researchers at washington state university published a study in communications biology in september that sought to better understand what goes on in the cells of hibernating grizzly bears\n",
      "1 the university is home to the wsu bear center , the only grizzly bear research center in the united states; it houses ## bears that were either raised in captivity or relocated to the center after being identified as problem bears in the wild\n",
      "2 researchers took samples from the liver , fat and muscle of six captive grizzly bears at three times during the year\n",
      "3 in the lab , a team of researchers analyzed the dna to understand the changes that occur in the cells over the course of the year\n",
      "4 the effect of hibernation on each tissue is different , said joanna kelley , an evolutionary biologist at washington state university and one of the paper 's authors\n",
      "5 hibernation is not just as simple as hibernating and not hibernating\n",
      "6 there are transitional things happening throughout the year\n",
      "7 the team found that the bears’ fatty tissues changed the most during hibernation , whereas the muscle tissue hardly changed at all\n",
      "8 the muscle cells remained active through the hibernation period , which might help explain why those tissues do not atrophy\n",
      "9 most surprising to heiko jansen , the study 's lead author , was that the bears’ fat contained a large number of genes that change their level of expression over the course of the year \n",
      "10 it 's in the thousands , he said\n",
      "11 in contrast , when dwarf lemurs in madagascar hibernate , only a few hundred genes in their fat tissues change their level of expression seasonally\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:19: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:20: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:34: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:35: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/long/model.py:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/beam_search_decoder.py:971: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 문제 생성 준비 중.. long_43.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. long_43.txt...\n",
      "3\n",
      "2\n",
      "1 번째는 The university is home to the wsu bear center, the only grizzly bear research center in the united states; it houses ## bears that were either raised in captivity or relocated to the center after being identified as problem bears in the wild. Researchers took samples from the liver, fat and muscle of six captive grizzly bears at three times during the year. In the lab, a team of researchers analyzed the dna to understand the changes that occur in the cells over the course of the year.\n",
      "2 번째는 The effect of hibernation on each tissue is different, said joanna kelley, an evolutionary biologist at washington state university and one of the paper 's authors. Hibernation is not just as simple as hibernating and not hibernating.\n",
      "3 번째는 There are transitional things happening throughout the year. The team found that the bears’ fatty tissues changed the most during hibernation, whereas the muscle tissue hardly changed at all. The muscle cells remained active through the hibernation period, which might help explain why those tissues do not atrophy.\n",
      "\n",
      "\n",
      "[43 ~ 45]. 다음 글을 읽고, 물음에 답하시오.\n",
      "-----------------------------------\n",
      "(a) a group of researchers at washington state university published a study in communications biology in september that sought to better understand what goes on in the cells of hibernating grizzly bears.\n",
      "-----------------------------------\n",
      "43. 주어진 글 (A)에 이어질 내용을 순서에 맞게 배열한 것으로 가장 적절한 것은?\n",
      "(B) The effect of hibernation on each tissue is different, said joanna kelley, an evolutionary biologist at washington state university and one of the paper 's authors. Hibernation is not just as simple as hibernating and not hibernating.\n",
      "(C) There are transitional things happening throughout the year. The team found that the bears’ fatty tissues changed the most during hibernation, whereas the muscle tissue hardly changed at all. The muscle cells remained active through the hibernation period, which might help explain why those tissues do not atrophy.\n",
      "(D) The university is home to the wsu bear center, the only grizzly bear research center in the united states; it houses ## bears that were either raised in captivity or relocated to the center after being identified as problem bears in the wild. Researchers took samples from the liver, fat and muscle of six captive grizzly bears at three times during the year. In the lab, a team of researchers analyzed the dna to understand the changes that occur in the cells over the course of the year.\n",
      "\n",
      "\n",
      "44. 다음 글의 주제로 가장 적절한 것은?\n",
      "① Muscle Tissue Transplant Changed In Hibernation.\n",
      "② Hibernation Works For Bears. Could It Work For Us, Too?.\n",
      "③ Transitional Year Of The Year.\n",
      "④ Muscle Cells Still In Hibernation Period.\n",
      "⑤ It 'S In The # , Years Of Year.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u는 사전에 없는 단어입니다\n",
      "build는 사전에 없는 단어입니다\n",
      "\n",
      "\n",
      "45. 윗글에 관한 내용으로 적절하지 않은 것은?\n",
      "① U # s researchers study year bears.\n",
      "② U # poor health university to build bears center..\n",
      "③ Dna analysis shows research.\n",
      "④ U # s scientists explore hibernation for.\n",
      "⑤ Hibernation now.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2019/11/15/science/hibernation-bears-winter-health.html'\n",
    "raw_txt_name = 'CSAT_43.txt'\n",
    "prepro_txt_name = 'prepro_43.txt'\n",
    "learned_txt_name = 'long_43.txt'\n",
    "num = 1\n",
    "num_word = 800\n",
    "\n",
    "def long_43(url, raw_txt_name, prepro_txt_name, learned_txt_name, num, num_word):\n",
    "    \n",
    "    text, title = crawl_news(url, raw_txt_name, num_word) # url과 raw_txt_name 파일명 입력\n",
    "    \n",
    "    #print(title)\n",
    "    \n",
    "    test_dict = bf_create_topic(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "      \n",
    "    shake_bot_topic(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # shake_bot_correct(learned_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict) # 5개의 선지가 나옴 \n",
    "    \n",
    "    main_set, first_set, second_set, third_set = make_order_sentence(test_dict)\n",
    "    \n",
    "    # main set 가공하기\n",
    "    main_set = main_set.capitalize()\n",
    "    \n",
    "    main_set = re.sub(' [.]', '.', main_set)\n",
    "    \n",
    "    main_set = re.sub(' ,', ',', main_set)\n",
    "    \n",
    "    main_set = re.sub('\\n', '', main_set)\n",
    "\n",
    "    # question set 가공하기\n",
    "    question_list = [first_set, second_set, third_set]\n",
    "    \n",
    "    answer_list = [first_set, second_set, third_set]\n",
    "    \n",
    "    alpha_list = ['(B)', '(C)', '(D)']\n",
    "    \n",
    "    random.shuffle(question_list) # 선지를 랜덤하게 섞기\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = alpha_list[ix] + ' ' + ' '.join(final_question)\n",
    "        \n",
    "        final_question = re.sub(' [.]', '.', final_question)\n",
    "        \n",
    "        final_question = re.sub(' ,', ',', final_question)\n",
    "        \n",
    "        final_question = re.sub('\\n', '', final_question)\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "        \n",
    "    \n",
    "    # 정답\n",
    "    \n",
    "    for ix, ans in enumerate(answer_list):\n",
    "        \n",
    "        ans = ' '.join(ans)\n",
    "        \n",
    "        ans = re.sub(' [.]', '.', ans)\n",
    "        \n",
    "        ans = re.sub(' ,', ',', ans)\n",
    "        \n",
    "        ans = re.sub('\\n', '', ans)\n",
    "        \n",
    "        print( '{} 번째는 {}'.format(ix+1, ans)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('[43 ~ 45]. 다음 글을 읽고, 물음에 답하시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(main_set)\n",
    "    print('-----------------------------------')\n",
    "    print('43. 주어진 글 (A)에 이어질 내용을 순서에 맞게 배열한 것으로 가장 적절한 것은?')\n",
    "    for question in question_list:\n",
    "        print(str(question))\n",
    "   \n",
    "    \n",
    "    question_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    #print(question_list)\n",
    "\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        replace_sentence = replace_sentence.title() + \".\"\n",
    "        \n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    #print(question_list)\n",
    "    \n",
    "    title = re.sub('\\n+', '', title)\n",
    "    \n",
    "    title = title.strip()\n",
    "    \n",
    "    title = title.title() + \".\"\n",
    "    \n",
    "    question_list.append(title)\n",
    "    \n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    print('\\n')\n",
    "    print('44. 다음 글의 주제로 가장 적절한 것은?')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "        \n",
    "    question_list, sort_rouge_list = for_rouge_test_correct(learned_txt_name, test_dict) # 5개의 선지가 나옴\n",
    "    \n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == num: # 오답 선지 1개 \n",
    "            \n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx != num: # 정답 선지 4개\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            #print(tag)\n",
    "            \n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # print(v_list)\n",
    "            \n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "                \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                    \n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                \n",
    "            \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "    \n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print('45. 윗글에 관한 내용으로 적절하지 않은 것은?')\n",
    "    \n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    long_43(url, raw_txt_name, prepro_txt_name, learned_txt_name, num, num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "from model import Model\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "from rouge import Rouge\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import ant_vocab\n",
    "import sym_vocab\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문 생성 및 문제 생성을 위한 텍스트 파일 생성 \n",
    "def crawl_news_summy(url, raw_txt_name, summy_txt_name, num_word):\n",
    "    # URL 기반 크롤링, 언어는 영어로 설정\n",
    "    news = Article(url, language = 'en')\n",
    "    news.download()\n",
    "    # HTML 파싱 적용\n",
    "    news.parse()\n",
    "    # 지정된 단어 갯수를 기준으로 textrank를 적용하여 파싱된 텍스트를 요약 (Rule base)\n",
    "    text = summarize(news.text, word_count = num_word) # 800자 기준으로 하기\n",
    "    title = news.title\n",
    "    title = title.lower()\n",
    "    \n",
    "    title = re.sub('\\([^)]*\\)', '', title) # 괄호 안 제거 \n",
    "    \n",
    "    # Textrank를 적용하여, 요약 지문에 해당하는 영역을 별도로 추출 \n",
    "    summy_text = summarize(news.text, word_count = 50)\n",
    "    \n",
    "    with open(raw_txt_name, 'w', encoding = 'utf-8') as f: \n",
    "        f.write(text)\n",
    "    \n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    # 크롤링한 raw data를 별도로 저장 \n",
    "    with open(summy_txt_name, 'w', encoding = 'utf-8') as f:\n",
    "        f.write(summy_text)\n",
    "        \n",
    "    summy_text = re.sub('\\n', ' ', summy_text)\n",
    "    \n",
    "    \n",
    "    return text, title, summy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈칸 지문 생성을 위한 함수를 별도로 정의\n",
    "def bf_create_long_summy(summy_txt_name, prepro_summy_name):\n",
    "    # 문장 번호: 문장 형식의 저장을 위해 변수 생성\n",
    "    test_dict = {}\n",
    "    \n",
    "    # 저장했었던 크롤링 본문 데이터 불러오기\n",
    "    with open(summy_txt_name, 'r', encoding = 'utf-8') as f:\n",
    "    # raw data 상태에서 전처리를 통해 모델에 들어갈 수 있는 데이터셋으로 변형\n",
    "        for idx, text in enumerate(f.readlines()):\n",
    "            # print(idx, text)\n",
    "            text = text.lower() # 소문자로 통일\n",
    "            text = re.sub('[.]', '', text) # 일단 온점 모두 제거\n",
    "            text = re.sub('[,]', ' ,', text) # 반점 하나의 단어로 인식\n",
    "            text = re.sub(\"\\’s\",' \\'s', text) # 소유격도 하나의 단어로 인식\n",
    "            text = re.sub(\"[0-9]\", '#', text) # 숫자를 #로 치환\n",
    "            #text = re.sub(\"'.+'\", '', text) # 작은 따옴표 안 고유명사 제거 \n",
    "            text = re.sub('[“”]', '', text) # 큰 따옴표 제거 \n",
    "            text = re.sub('\\n', '', text) # 줄 바꿈표 제거 \n",
    "            print(idx, text)\n",
    "        \n",
    "            # 사전에 저장하기\n",
    "            test_dict[idx] = text\n",
    "    # 전처리된 데이터를 별도의 파일로 저장\n",
    "    with open(prepro_summy_name, 'w', encoding = 'utf-8') as f: # 중첩 형식 사용해보기\n",
    "        # 줄 바꿈을 기준으로 문장을 구분하여 저장\n",
    "        for i in range(len(test_dict)):\n",
    "            test_dict[i] = test_dict[i] + \" \" + \".\" + '\\n'\n",
    "            f.write(test_dict[i])\n",
    "    \n",
    "    return test_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shake_bot_long(filename, valid_article_path):\n",
    "    # 설정된 세팅 값 불러오기\n",
    "    with open(\"args.pickle\", \"rb\") as f:\n",
    "        args = pickle.load(f)\n",
    "        \n",
    "    # 실행시 최초의 상태로 초기화하여 모델 재사용이 가능하도록 함\n",
    "    tf.reset_default_graph()\n",
    "    # 학습된 단어 사전과 본문 최대 길이와 요약 최대 길이 설정 값을 불러옴\n",
    "    print(\"셰봇이 단어 사전 불러오는 중...\")\n",
    "    word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "    # 불러온 설정 값에 따라서 검증 데이터셋을 생성\n",
    "    print(\"셰봇이 기본 설정 값 초기화 중...\")\n",
    "    valid_x = build_dataset(\"valid\", vaild_article_path, word_dict, article_max_len, summary_max_len, args.toy)\n",
    "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # 학습된 체크포인트와 모델을 불러오기 \n",
    "        print(\"Loading saved model...\")\n",
    "        model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True) # 검증 단계\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # 검증을 진행할 배치 사이즈 설정\n",
    "        batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "        \n",
    "        # 검증 단계인 만큼 인코더 인풋 부분만 고려하여 학습 진행\n",
    "        print(\"셰봇이 수능 요약 문제 생성 준비 중.. {}...\".format(filename))\n",
    "        for batch_x, _ in batches:\n",
    "            # 문장 -> 문장 예측이 가능하도록 데이터 분할 \n",
    "            batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "            # 검증을 위한 배치에 따라 모델 및 데이터 셋 설정 준비 \n",
    "            valid_feed_dict = {\n",
    "                model.batch_size: len(batch_x),\n",
    "                model.X: batch_x,\n",
    "                model.X_len: batch_x_len,\n",
    "            }\n",
    "            # Prediction을 목적으로 학습된 모델을 검증을 위한 설정 값을 적용하여 예측 결과 생성\n",
    "            prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "            # 인덱스 -> 단어 사전을 통해 학습된 결과를 단어로 표현.\n",
    "            prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "            \n",
    "            # 생성된 예측 결과물을 문장 단위로 저장 \n",
    "            with open(filename, \"w\") as f:\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    print(\" \".join(summary), file=f)\n",
    "                    \n",
    "            #with open('./rouge_test/sys_dir/{}.txt'.format(filename), \"w\") as f:\n",
    "                #for line in prediction_output:\n",
    "                    #summary = list()\n",
    "                    #for word in line:\n",
    "                        #if word == \"</s>\":\n",
    "                            #break\n",
    "                        #if word not in summary:\n",
    "                            #summary.append(word)\n",
    "                    #print(\" \".join(summary), file=f)\n",
    "\n",
    "        print('셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. {}...'.format(filename))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과물 중 <unk> 부분 처리\n",
    "def delete_unk(content, unk_sentence):\n",
    "    \n",
    "    container = []\n",
    "    \n",
    "    text = open(content, 'r', encoding = 'utf-8').read()\n",
    "    \n",
    "    # 각 단어에 대해서 품사 태깅\n",
    "    tagged_list = pos_tag(word_tokenize(text))\n",
    "    \n",
    "    # 명사 부분만 활용\n",
    "    nnp_list = [t[0] for t in tagged_list if (t[1] == \"NNP\") or (t[1] == \"NN\")]\n",
    "    \n",
    "    fd_names = FreqDist(nnp_list)\n",
    "    \n",
    "    # 빈도수 기반으로 Unk 처리하기\n",
    "    freq = fd_names.most_common(2)\n",
    "    \n",
    "    #for tagged in tagged_list:\n",
    "        \n",
    "        #if tagged[1] == 'NN' or 'NNP':\n",
    "            \n",
    "            #container.append(tagged[0]) # 단어만 추출\n",
    "    \n",
    "    \n",
    "    # unk 부분을 처리하는 방법 중 하나로 최고 빈도수를 기반으로 하는 방식\n",
    "    ## 현재 여기서는 정확도를 해치는 것으로 판단 \n",
    "    #replace_sentence = re.sub('< unk >', freq[0][0], unk_sentence)\n",
    "    \n",
    "    # 다중 숫자에 대한 표현은 Several로 대체 \n",
    "    replace_sentence = re.sub('# nd', 'Several', unk_sentence)\n",
    "    \n",
    "    #replace_sentence = re.sub('# nd', 'Several', replace_sentence)\n",
    "    \n",
    "    # 불필요한 줄 바꿈표 제거 \n",
    "    replace_sentence = re.sub('\\n', '', replace_sentence)\n",
    "    \n",
    "    return replace_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 it works like this: employers , known as requesters , post batches of what are called human intelligence tasks , or hits , on mechanical turk 's website\n",
      "1 while amazon pays all of its american employees at least $## an hour and favors raising the minimum wage , the company declined numerous requests to comment about the pay policy for turkers or anything else regarding mechanical turk\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 문제 생성 준비 중.. summy_40.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. summy_40.txt...\n",
      "(A): asked (B): raise\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "40. 다음 글의 내용을 한 문장으로 요약하고자 한다. 빈칸 (A), (B)에 들어갈 말로 가장 적절한 것은?\n",
      "-----------------------------------\n",
      "I had just earned another 5 cents on a digital work marketplace run by Amazon called Mechanical Turk. It works like this: Employers, known as requesters, post batches of what are called Human Intelligence Tasks, or HITs, on Mechanical Turk’s website. ⚙️ Try Your Hand at Turking Throughout this story we’ll give you the chance to do HITs (Human Intelligence Tasks) that The Times has devised based on ones that novice workers like me encounter on Mechanical Turk. Though they probably overrepresented novice turkers like me who do the lowest-paying tasks, the paper’s authors concluded that if you count time spent looking for tasks and working on tasks that came to nothing, the median turker’s hourly wage was $1.77. It has ignored turkers’ pleas to mandate higher wages, even as it takes a cut of each transaction ranging from 17 to 50 percent; a requester posting a 1-cent HIT pays one penny to the turker and another to Amazon. While Amazon pays all of its American employees at least $15 an hour and favors raising the minimum wage, the company declined numerous requests to comment about the pay policy for turkers or anything else regarding Mechanical Turk. While the pay is “really low,” she said, she prefers turking to fast food for its freedom — freedom from having to wear a uniform, freedom to spend time at home with her mother, freedom to watch videos between tasks. Ms. Milland, the turker advocate, said the biggest problem on MTurk was that requesters could decline to pay turkers by “rejecting” their submissions, but still keep the work. “I’ve known a requester to be open about the fact that they automatically reject 10 percent of jobs to pay for Amazon fees,” she said. Action Accept & Work This task is based on actual tasks turkers complete on the Mechanical Turk platform. Action Accept & Work This task is based on actual tasks turkers complete on the Mechanical Turk platform. Action Accept & Work This task is based on actual tasks turkers complete on the Mechanical Turk platform. Action Accept & Work This task is based on actual tasks turkers complete on the Mechanical Turk platform. Professor Ipeirotis of N.Y.U. said his research indicated that Amazon itself was one of the biggest requesters and that it posted tasks under many different aliases. Amazon refused to say whether it posts tasks on Mechanical Turk.\n",
      "-----------------------------------\n",
      "U # s firm ___(a)___ to ___(b)___ minimum wage\n",
      "\n",
      "\n",
      "① (A): decrease (B): asked\n",
      "② (A): raise (B): decrease\n",
      "③ (A): asked (B): answer\n",
      "④ (A): asked (B): raise\n",
      "⑤ (A): decrease (B): asked\n"
     ]
    }
   ],
   "source": [
    "# 요약 지문 생성\n",
    "url = 'https://www.nytimes.com/interactive/2019/11/15/nyregion/amazon-mechanical-turk.html?action=click&module=Top%20Stories&pgtype=Homepage'\n",
    "num_word = 400\n",
    "raw_txt_name = 'CSAT_40.txt'\n",
    "summy_txt_name = 'summy_40_txt'\n",
    "prepro_summy_name = 'prepro_summy_40.txt'\n",
    "learned_txt_name = 'summy_40.txt'\n",
    "\n",
    "def summy_40(url, raw_txt_name, summy_txt_name, prepro_summy_name, learned_txt_name, num_word):\n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기\n",
    "    text, title, summy_text = crawl_news_summy(url, raw_txt_name, summy_txt_name, num_word) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_long_summy(summy_txt_name, prepro_summy_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_long(learned_txt_name, prepro_summy_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    summy_list = list()\n",
    "    \n",
    "    v_list = list()\n",
    "    \n",
    "    # 요약 지문에 대해서 parapharasing 적용\n",
    "    with open(learned_txt_name, 'r', encoding = 'utf-8') as f:\n",
    "        \n",
    "        for summy_text in f.readlines():\n",
    "            \n",
    "            summy_text = delete_unk(prepro_summy_name, summy_text)\n",
    "            \n",
    "            token = word_tokenize(summy_text)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            \n",
    "            # 요약 지문에 대해서 동사 부분 빈칸 처리\n",
    "            for ix, t in enumerate(tag): \n",
    "                \n",
    "                if ((t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")):\n",
    "                        \n",
    "                    v_list.append(t[0])\n",
    "                    \n",
    "                    if len(v_list) == 1:\n",
    "                    \n",
    "                        token[ix] = '___(A)___'\n",
    "                    \n",
    "                    elif len(v_list) == 2:\n",
    "                        \n",
    "                        token[ix] = '___(B)___'\n",
    "                        \n",
    "                        \n",
    "                if len(v_list) == 2:\n",
    "                    \n",
    "                    break \n",
    "                    \n",
    "            token = ' '.join(token)\n",
    "            \n",
    "            summy_text = token.capitalize() ## 요약 지문 완성\n",
    "            \n",
    "    \n",
    "    # 정답 지문 생성 \n",
    "    sym_words_list = list()\n",
    "    \n",
    "    for v in v_list:\n",
    "        # 유의어 사전을 활용\n",
    "        sym_dict = sym_vocab.crawling_dict(v)\n",
    "        \n",
    "        sym_list = sym_dict[v]\n",
    "        # 유의어가 존재하는 경우\n",
    "        if sym_list == list:\n",
    "        \n",
    "            random.shuffle(sym_list)\n",
    "            \n",
    "            sym_words_list.append(sym_list[0])\n",
    "        # 유의어가 존재하지 않는 경우\n",
    "        elif sym_list != list:\n",
    "            \n",
    "            sym_words_list.append(sym_list)\n",
    "        \n",
    "    \n",
    "    # 오답 지문 생성\n",
    "    ant_words_list = list()\n",
    "    \n",
    "    for v in v_list:\n",
    "        \n",
    "        ant_word = ant_vocab.crawling_dict(v)\n",
    "        \n",
    "        ant_words_list.append(ant_word)\n",
    "    \n",
    "        \n",
    "    \n",
    "    question_list = list()       \n",
    "    # 정답 부분\n",
    "    answer = '(A): ' + v_list[0] + ' ' + '(B): ' + v_list[1]\n",
    "    print(answer)\n",
    "    \n",
    "    # 정답 이외의 나머지 4개 선지 제작 \n",
    "    question_list.append(answer)\n",
    "    \n",
    "    question_1 = '(A): ' + sym_words_list[1] + ' ' + '(B): ' + ant_words_list[1]\n",
    "    \n",
    "    question_list.append(question_1) \n",
    "    \n",
    "    question_2 = '(A): ' + sym_words_list[0] + ' ' + '(B): ' + ant_words_list[0]\n",
    "    \n",
    "    question_list.append(question_2) \n",
    "    \n",
    "    question_3 = '(A): ' + ant_words_list[1] + ' ' + '(B): ' + sym_words_list[0]\n",
    "    \n",
    "    question_list.append(question_3) \n",
    "    \n",
    "    question_4 = '(A): ' + ant_words_list[1] + ' ' + '(B): ' + sym_words_list[0]\n",
    "    \n",
    "    question_list.append(question_4) \n",
    "       \n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('40. 다음 글의 내용을 한 문장으로 요약하고자 한다. 빈칸 (A), (B)에 들어갈 말로 가장 적절한 것은?')\n",
    "    print('-----------------------------------')\n",
    "    print(text) # 지문 600자\n",
    "    print('-----------------------------------')\n",
    "    print(summy_text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    summy_40(url, raw_txt_name, summy_txt_name, prepro_summy_name, learned_txt_name, num_word)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "from model import Model\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "from rouge import Rouge\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sym_vocab\n",
    "import ant_vocab\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# download \n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문 생성 및 문제 생성을 위한 텍스트 파일 생성\n",
    "def crawl_news(url, raw_txt_name, num_words):\n",
    "    # URL 기반 크롤링, 언어는 영어로 설정\n",
    "    news = Article(url, language = 'en') # URL 가져오기\n",
    "    news.download() # 해당 url의 html을 가져오기\n",
    "    # HTML 파싱 적용\n",
    "    news.parse() \n",
    "    # 지정된 단어 갯수를 기준으로 Textrank를 적용하여 파싱된 텍스트를 요약 (Rule base)\n",
    "    text = summarize(news.text, word_count = num_words) # 600자 기준으로 하기\n",
    "    title = news.title # 파싱한 html에서 title에 해당하는 부분 추출하기\n",
    "    title = title.lower() # title에 대해서 전부 소문자화 \n",
    "    \n",
    "    title = re.sub('\\([^)]*\\)', '', title) # 괄호를 포함한 문자열 제거 \n",
    "    \n",
    "    # 크롤링한 raw data를 별도로 저장\n",
    "    with open(raw_txt_name, 'w', encoding = 'utf-8') as f: # 크롤링한 raw data를 저장하기\n",
    "        f.write(text)\n",
    "        \n",
    "    # 크롤링한 뉴스 기사 문단 분리 없이 변수 할당\n",
    "    text = re.sub('\\n', ' ', text) # raw data에서 불필요한 줄바꿈표 제거 (상대적일듯)\n",
    "\n",
    "    return text, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일치 지문 생성을 위한 함수를 별도로 정의\n",
    "def bf_create_correct(raw_txt_name, prepro_txt_name):\n",
    "    # 문장 번호: 문장 형식의 저장을 위해 변수 생성\n",
    "    test_dict = {}\n",
    "    \n",
    "    # 저장했었던 크롤링 본문 데이터 불러오기\n",
    "    with open(raw_txt_name, 'r', encoding = 'utf-8') as f:\n",
    "    # raw data 상태에서 전처리를 통해 모델에 들어갈 수 있는 데이터셋으로 변형\n",
    "        for idx, text in enumerate(f.readlines()):\n",
    "            # print(idx, text)\n",
    "            text = text.lower() # 소문자로 통일\n",
    "            text = re.sub('[.]', '', text) # 일단 온점 모두 제거\n",
    "            text = re.sub('[,]', ' ,', text) # 반점 하나의 단어로 인식\n",
    "            text = re.sub(\"\\’s\",' \\'s', text) # 소유격도 하나의 단어로 인식\n",
    "            text = re.sub(\"[0-9]\", '#', text) # 숫자를 #로 치환\n",
    "            #text = re.sub(\"'.+'\", '', text) # 작은 따옴표 안 고유명사 제거 \n",
    "            text = re.sub('[“”]', '', text) # 큰 따옴표 제거 \n",
    "            text = re.sub('\\n', '', text) # 줄 바꿈표 제거 \n",
    "            print(idx, text)\n",
    "        \n",
    "            # 사전에 저장하기\n",
    "            test_dict[idx] = text ## {1: '텍스트', 2: '텍스트'}\n",
    "    \n",
    "    # 전처리된 데이터를 별도의 파일로 저장 \n",
    "    with open(prepro_txt_name, 'w', encoding = 'utf-8') as f: # 전처리한 데이터를 저장하기 \n",
    "        # 줄 바꿈을 기준으로 문장을 구분하여 저장 \n",
    "        for i in range(len(test_dict)):\n",
    "            test_dict[i] = test_dict[i] + \" \" + \".\" + '\\n'\n",
    "            f.write(test_dict[i])\n",
    "    \n",
    "    return test_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 요약 학습 데이터 불러오기 \n",
    "def shake_bot_correct(filename, vaild_article_path):\n",
    "    # 설정된 세팅 값 불러오기\n",
    "    with open(\"args.pickle\", \"rb\") as f:\n",
    "        args = pickle.load(f)\n",
    "        \n",
    "    # 실행시 최초의 상태로 초기화하여 모델 재사용이 가능하도록 함\n",
    "    tf.reset_default_graph()\n",
    "    # 학습된 단어 사전과 본문 최대 길이와 요약 최대 길이 설정 값을 불러옴\n",
    "    print(\"셰봇이 단어 사전 불러오는 중...\")\n",
    "    word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "    # 불러온 설정 값에 따라서 검증 데이터셋을 생성\n",
    "    print(\"셰봇이 기본 설정 값 초기화 중...\")\n",
    "    valid_x = build_dataset(\"valid\", vaild_article_path, word_dict, article_max_len, summary_max_len, args.toy)\n",
    "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # 학습된 체크포인트와 모델을 불러오기 \n",
    "        print(\"Loading saved model...\")\n",
    "        model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True) # 검증 단계\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # 검증을 진행할 배치 사이즈 설정\n",
    "        batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "        \n",
    "        # 검증 단계인 만큼 인코더 인풋 부분만 고려하여 학습 진행\n",
    "        print(\"셰봇이 수능 일치 문제 생성 준비 중.. {}...\".format(filename))\n",
    "        for batch_x, _ in batches:\n",
    "            # 문장 -> 문장 예측이 가능하도록 데이터 분할 \n",
    "            batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "            # 검증을 위한 배치에 따라 모델 및 데이터 셋 설정 준비 \n",
    "            valid_feed_dict = {\n",
    "                model.batch_size: len(batch_x),\n",
    "                model.X: batch_x,\n",
    "                model.X_len: batch_x_len,\n",
    "            }\n",
    "            # Prediction을 목적으로 학습된 모델을 검증을 위한 설정 값을 적용하여 예측 결과 생성\n",
    "            prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "            # 인덱스 -> 단어 사전을 통해 학습된 결과를 단어로 표현.\n",
    "            prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "            \n",
    "            # 생성된 예측 결과물을 문장 단위로 저장 \n",
    "            with open(filename, \"w\") as f:\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    print(\" \".join(summary), file=f)\n",
    "                    \n",
    "            #with open('./rouge_test/sys_dir/{}.txt'.format(filename), \"w\") as f:\n",
    "                #for line in prediction_output:\n",
    "                    #summary = list()\n",
    "                    #for word in line:\n",
    "                        #if word == \"</s>\":\n",
    "                            #break\n",
    "                        #if word not in summary:\n",
    "                            #summary.append(word)\n",
    "                    #print(\" \".join(summary), file=f)\n",
    "\n",
    "        print('셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. {}...'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 잘된 문장을 선별하여 선지로 만들기 위해 함수 생성\n",
    "def for_rouge_test(summy, label):\n",
    "    rouge_dict = {}\n",
    "    rouge_list = []\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # 학습 결과로 예측한 결과물을 불러오기\n",
    "    with open(summy, 'r', encoding = 'utf-8') as f:\n",
    "            \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            # 실제 문장 - 예측 결과에서 학습의 정도를 Rouge-r과 Rouge-p로 검증\n",
    "            scores = rouge.get_scores(line, label[idx])\n",
    "            \n",
    "            p_score = scores[0][\"rouge-1\"][\"p\"] \n",
    "            r_score = scores[0][\"rouge-1\"][\"r\"] \n",
    "            \n",
    "            # 해당 조건은 재검토가 필요할 듯 \n",
    "            if r_score > 0: \n",
    "                \n",
    "                # Rouge-r 사용\n",
    "                r_idx_score = (idx, r_score)            \n",
    "                \n",
    "                rouge_list.append(r_idx_score)\n",
    "                # print(rouge_list)\n",
    "            \n",
    "                rouge_dict[idx] = line\n",
    "            \n",
    "        # 내림차순으로 가장 점수가 높은 문장을 순서로 리스트 배열\n",
    "        rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[1], reverse=True)\n",
    "        # 상위 5개 문장 추출하기\n",
    "        rouge_list = rouge_list[:5]\n",
    "        # 오름차순으로 인덱스 순서대로 다시 재배열\n",
    "        sort_rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[0], reverse=False)\n",
    "                \n",
    "        question_list = []\n",
    "    \n",
    "        for ix in sort_rouge_list:\n",
    "            \n",
    "            question_list.append(rouge_dict[ix[0]])\n",
    "    # 상위 5개 문장에 대해서 선지로 활용할 수 있도록 리턴      \n",
    "    return question_list, sort_rouge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과물 중 <unk> 부분 처리 \n",
    "def delete_unk(content, unk_sentence):\n",
    "    \n",
    "    container = []\n",
    "    \n",
    "    text = open(content, 'r', encoding = 'utf-8').read()\n",
    "    \n",
    "    # 각 단어에 대해서 품사 태깅\n",
    "    tagged_list = pos_tag(word_tokenize(text))\n",
    "    \n",
    "    # 명사 부분만 활용\n",
    "    nnp_list = [t[0] for t in tagged_list if (t[1] == \"NNP\") or (t[1] == \"NN\")]\n",
    "    \n",
    "    fd_names = FreqDist(nnp_list)\n",
    "    \n",
    "    # 빈도수 기반으로 unk 처리하기\n",
    "    freq = fd_names.most_common(2)\n",
    "    \n",
    "    #for tagged in tagged_list:\n",
    "        \n",
    "        #if tagged[1] == 'NN' or 'NNP':\n",
    "            \n",
    "            #container.append(tagged[0]) # 단어만 추출\n",
    "    \n",
    "    \n",
    "    # unk 부분을 처리하는 방법 중 하나로 최고 빈도수를 기반으로 하는 방식\n",
    "    replace_sentence = re.sub('< unk >', freq[0][0], unk_sentence)\n",
    "    # 다중 숫자에 대한 표현은 Several로 대체 \n",
    "    replace_sentence = re.sub('# nd', 'Several', replace_sentence)\n",
    "    # 불필요한 줄 바꿈표 제거 \n",
    "    replace_sentence = re.sub('\\n', '', replace_sentence)\n",
    "    \n",
    "    return replace_sentence\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드넷 반의어 사전 활용하기 \n",
    "def antonym_dict(word):\n",
    "    syn = []\n",
    "    ant = []\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            syn.append(lemma.name())    #add the synonyms\n",
    "            if lemma.antonyms():    #When antonyms are available, add them into the list\n",
    "                ant.append(lemma.antonyms()[0].name())\n",
    "    # print('Synonyms: ' + str(syn))\n",
    "    print('Antonyms: ' + str(ant))\n",
    "    \n",
    "    #random.shuffle(ant)\n",
    "    \n",
    "    if len(ant) > 0:\n",
    "    \n",
    "        return ant[0] # string이 최종\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오답 선지 제작하는 함수 \n",
    "def make_wrong_question(replace_sentence): # 딱 하나만 들어가야 함.\n",
    "    # 선지로 들어가는 문장을 어절 단위로 나누기\n",
    "    token = word_tokenize(replace_sentence)\n",
    "    # 각 어절에 품사 태깅하기 \n",
    "    tagged_list = pos_tag(token)\n",
    "    # 형용사, 부사, 동사 일부에 대해서 반의어로 변경\n",
    "    va_list = [t[0] for t in tagged_list if (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\")\n",
    "               or (t[1] == \"RB\") or (t[1] == \"RBS\") or (t[1] == \"RBR\") or (t[1] == \"VB\") or (t[1] == \"VBD\")]\n",
    "    \n",
    "    \n",
    "    for va in va_list:\n",
    "        # 반의어 사전에서 반의어 목록 불러오기\n",
    "        para = ant_vocab.crawling_dict(va)\n",
    "        # 반의어가 존재하는지 유무가 결정적\n",
    "        if len(para) > 0:\n",
    "            # 있다면? 바꾸어주기 \n",
    "            token[token.index(va)] = para  \n",
    "            \n",
    "    # 리스트 > 문자열로 연결하기\n",
    "    replace_sentence = ' '.join(token)\n",
    "    # 최초 문자 대문자로\n",
    "    replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "    \n",
    "    return replace_sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 albert was the first patient in the world to receive the antibiotic — penicillin\n",
      "1 for the past year , matt 's been talking to health experts to find out if we are reaching the end of the antibiotic era\n",
      "2 is our fate sealed? first off , i don’t think people respect bacteria enough this is ellen silbergeld , one of the leading scientists studying antibiotic resistance\n",
      "3 no the cdc got our attention today with a warning about what it calls ‘nightmare bacteria’ these are bacteria that are resistant to most , if not all , antibiotics when we take antibiotics to kill infections , some bacteria survive\n",
      "4 so every time we take an antibiotic , we risk creating stronger , more resistant bacteria\n",
      "5 and stronger , more resistant bacteria means less and less effective antibiotics\n",
      "6 drug-resistant bacteria have never been able to travel the world as fast as they do today and that 's just part of the problem\n",
      "7 you should know that about ## percent of antibiotic production in this country goes into agriculture why on earth did somebody think putting antibiotics in agriculture was a great idea? we’ve said , hey , look , cram these animals together\n",
      "8 and you can make money off of that nobody was making the connection between feeding animals antibiotics and the fact that the food would be carrying drug-resistant bacteria so ellen did a study\n",
      "9 and she found that poultry raised with antibiotics had nine times as much drug-resistant bacteria on it\n",
      "10 organic agriculture lauds the use of animal manure unless you’re just a complete , ‘i’m a vegan , and i only hang out with vegans , and i eat sterilized vegetables ,’ you know , it 's very likely that you’re picking up the same bacteria resistant bacteria seep into the groundwater , fly off the back of livestock trucks and hitch a ride home on the hands of farm workers , all of which makes trying to pinpoint exactly where resistant bacteria is originating extremely difficult\n",
      "11 it was a dead end ### people sick , ## hospitalizations and zero access for health officials to investigate the farms\n",
      "12 drug-resistant bacteria is a huge problem\n",
      "13 five days after he started recovering , the hospital ran out of the new drug , and mr alexander died\n",
      "14 today , we don’t have to worry about antibiotics running out\n",
      "15 — want to know why a metro health department didn’t shut down a restaurant — it 's a very resistant bacteria — we really need to change the way we use antibiotics\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:19: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:20: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:34: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:35: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/correct/model.py:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/beam_search_decoder.py:971: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 일치 문제 생성 준비 중.. correct_25.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. correct_25.txt...\n",
      "drug-resistant는 사전에 없는 단어입니다\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25. 다음 글에 일치하지 않은 것을 고르시오.\n",
      "-----------------------------------\n",
      "Albert was the first patient in the world to receive the antibiotic — penicillin. For the past year, Matt’s been talking to health experts to find out if we are reaching the end of the antibiotic era. Is our fate sealed?” “First off, I don’t think people respect bacteria enough.” This is Ellen Silbergeld, one of the leading scientists studying antibiotic resistance. No.” “The C.D.C. got our attention today with a warning about what it calls ‘nightmare bacteria.’” “These are bacteria that are resistant to most, if not all, antibiotics.” When we take antibiotics to kill infections, some bacteria survive. So every time we take an antibiotic, we risk creating stronger, more resistant bacteria. And stronger, more resistant bacteria means less and less effective antibiotics. “Drug-resistant bacteria have never been able to travel the world as fast as they do today.” And that’s just part of the problem. “You should know that about 80 percent of antibiotic production in this country goes into agriculture.” “Why on earth did somebody think putting antibiotics in agriculture was a great idea?” “We’ve said, hey, look, cram these animals together. And you can make money off of that.” “Nobody was making the connection between feeding animals antibiotics and the fact that the food would be carrying drug-resistant bacteria.” So Ellen did a study. And she found that poultry raised with antibiotics had nine times as much drug-resistant bacteria on it. Organic agriculture lauds the use of animal manure.” “Unless you’re just a complete, ‘I’m a vegan, and I only hang out with vegans, and I eat sterilized vegetables,’ you know, it’s very likely that you’re picking up the same bacteria.” Resistant bacteria seep into the groundwater, fly off the back of livestock trucks and hitch a ride home on the hands of farm workers, all of which makes trying to pinpoint exactly where resistant bacteria is originating extremely difficult. It was a dead end.” 192 people sick, 30 hospitalizations and zero access for health officials to investigate the farms. “Drug-resistant bacteria is a huge problem. Five days after he started recovering, the hospital ran out of the new drug, and Mr. Alexander died. Today, we don’t have to worry about antibiotics running out. “— want to know why a metro health department didn’t shut down a restaurant —” “It’s a very resistant bacteria —” “We really need to change the way we use antibiotics.\n",
      "\n",
      "\n",
      "① First antibiotic bacteria penicillin.\n",
      "② Drug-resistant bacteria break greater helpless..\n",
      "③ Poultry found with antibiotics.\n",
      "④ Drug-resistant bacteria a big problem.\n",
      "⑤ Hospital runs out of new drug.\n"
     ]
    }
   ],
   "source": [
    "# 내용 일치 문제 생성\n",
    "\n",
    "url = 'https://www.nytimes.com/2019/11/13/health/candida-auris-resistant-hospitals.html'\n",
    "raw_txt_name = 'CSAT_25.txt'\n",
    "prepro_txt_name = 'prepro_25.txt'\n",
    "learned_txt_name = 'correct_25.txt'\n",
    "num = 1\n",
    "# 지문의 길이는 뉴스 기사 길이에 따라 유동적으로 설정하기\n",
    "num_words = 400\n",
    "\n",
    "def correct_25(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_correct(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_correct(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    # Rouge Score에 따라 선지 생성 \n",
    "    question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict) # 5개의 선지가 나옴 \n",
    "\n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == num: # 오답 선지 1개 선택\n",
    "            # 오답으로 변화시키기\n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx != num: # 정답 선지 4개 선택\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사들에 대해서만\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # 유의어 사전을 바탕으로 유사한 의미를 지닌 단어로 변경\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v] # 유의어가 없는 경우가 발생\n",
    "                \n",
    "                if paraphrasing_list == list: # 유의어 사전에서 유사어가 있는 경우\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                elif paraphrasing_list != list: # 유사어가 없는 경우 \n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "    \n",
    "        # 오답 선지 1개, 정답 선지 4개 리스트로 배열\n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    # 넘버링을 포함한 선지로 가공\n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('25. 다음 글에 일치하지 않은 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    correct_25(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 shortly after pitcher jacob degrom signed a contract extension worth $#### million with the mets on the eve of opening day in march , his father , tony , waited for him inside the diplomat room off the lobby at the ritz-carlton in arlington , va\n",
      "1 tony degrom considered his son 's accomplishments a day before he would start the mets’ opener : rookie of the year in #### , world series runner-up in #### and the #### cy young award winner\n",
      "2 the elder degrom believed his son 's most recent accomplishment would not be his peak\n",
      "3 i think he 's capable of winning a few more cy youngs , tony degrom said\n",
      "4 if you look at his history , he has a lot less wear on him than most guys his age , so i think he 's capable of winning a few more’’\n",
      "5 the father proved prophetic wednesday night\n",
      "6 jacob degrom , who battled through a tough stretch in april and overcame what manager mickey callaway called a barking elbow , won the national league cy young award and became the ##th pitcher to claim the prize in consecutive years\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 일치 문제 생성 준비 중.. correct_26.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. correct_26.txt...\n",
      "not는 사전에 없는 단어입니다\n",
      "more는 사전에 없는 단어입니다\n",
      "free는 사전에 없는 단어입니다\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26. 다음 글에 일치하는 것을 고르시오.\n",
      "-----------------------------------\n",
      "Shortly after pitcher Jacob deGrom signed a contract extension worth $137.5 million with the Mets on the eve of opening day in March, his father, Tony, waited for him inside the Diplomat Room off the lobby at the Ritz-Carlton in Arlington, Va. Tony deGrom considered his son’s accomplishments a day before he would start the Mets’ opener : rookie of the year in 2014, World Series runner-up in 2015 and the 2018 Cy Young Award winner. The elder deGrom believed his son’s most recent accomplishment would not be his peak. “I think he’s capable of winning a few more Cy Youngs,” Tony deGrom said. “If you look at his history, he has a lot less wear on him than most guys his age, so I think he’s capable of winning a few more.’’ The father proved prophetic Wednesday night. Jacob deGrom, who battled through a tough stretch in April and overcame what Manager Mickey Callaway called a “barking” elbow, won the National League Cy Young Award and became the 11th pitcher to claim the prize in consecutive years.\n",
      "\n",
      "\n",
      "① Degrom has a antiquated look for the mets..\n",
      "② Degrom opener in.\n",
      "③ Degrom 's not to cease his peak..\n",
      "④ Degrom says he can defeat more free..\n",
      "⑤ Degrom wins private league cy experienced award..\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.nytimes.com/2019/11/13/sports/baseball/jacob-degrom-cy-young.html'\n",
    "raw_txt_name = 'CSAT_26.txt'\n",
    "prepro_txt_name = 'prepro_26.txt'\n",
    "learned_txt_name = 'correct_26.txt'\n",
    "num = 1\n",
    "num_words = 400\n",
    "def correct_26(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_correct(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_correct(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    # Rouge Score에 따라 선지 생성 \n",
    "    question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict) # 5개의 선지가 나옴 \n",
    "    \n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasin\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx != num: # 오답 선지 4개 \n",
    "            # 오답으로 변화시키기\n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx == num: # 정답 선지 1개\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사들에 대해서만\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # 유의어 사전을 바탕으로 유사한 의미를 지닌 단어로 변경\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v] # 유의어가 없는 경우가 발생\n",
    "                \n",
    "                if paraphrasing_list == list: # 유의어 사전에서 유사어가 있는 경우\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                elif paraphrasing_list != list: # 유사어가 없는 경우\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                \n",
    "            \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "        \n",
    "        # 오답 선지 4개, 정답 선지 1개 리스트로 배열\n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    # 넘버링을 포함한 선지로 가공\n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('26. 다음 글에 일치하는 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    correct_26(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you’re going to fly for nearly ## hours through multiple time zones dressed in a pair of kangaroo-themed pajamas , jolting in and out of sleep in contravention of your normal circadian rhythm , you should take it easy on the medication\n",
      "1 no one wants to go crazy in a metal tube ## ,### feet above the pacific\n",
      "2 on the other hand , suspecting that you have fallen into a rift in the space-time continuum itself is perhaps as reasonable a response as any to the longest (so far) commercial flight in the world\n",
      "3 so there i was last month , six or so hours into qantas 's first-ever nonstop flight from new york city to sydney\n",
      "4 it was # am new york time , which made it no o’clock in the tiny upscale refugee camp created by the airline\n",
      "5 i had been suffering from congestion , the kind that migrates around your sinuses and then becomes an infection in your ear\n",
      "6 while i was no longer contagious , there was some issue about the future of my ability to hear\n",
      "7 the internet did not have good news\n",
      "8 flying with an ear infection doesn’t always result in a ruptured eardrum , one website said\n",
      "9 basically , i had been taking decongestants since midafternoon\n",
      "10 i felt like a junkie in a gritty tv show about times square in the ####s , nervous and sweaty and incoherent even as i was beset by an achy , leaden inertia\n",
      "11 soon the lights would go down , part of the airline 's next planned group activity (sleeping) and i would make perhaps the gravest pharmaceutical error of my adult life\n",
      "12 but that was still in the future\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 일치 문제 생성 준비 중.. correct_27.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. correct_27.txt...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "27. 다음 글에 일치하지 않은 것을 고르시오.\n",
      "-----------------------------------\n",
      "If you’re going to fly for nearly 20 hours through multiple time zones dressed in a pair of kangaroo-themed pajamas, jolting in and out of sleep in contravention of your normal circadian rhythm, you should take it easy on the medication. No one wants to go crazy in a metal tube 40,000 feet above the Pacific. On the other hand, suspecting that you have fallen into a rift in the space-time continuum itself is perhaps as reasonable a response as any to the longest (so far) commercial flight in the world. So there I was last month, six or so hours into Qantas’s first-ever nonstop flight from New York City to Sydney. It was 3 a.m. New York time, which made it No O’clock in the tiny upscale refugee camp created by the airline. I had been suffering from congestion, the kind that migrates around your sinuses and then becomes an infection in your ear. While I was no longer contagious, there was some issue about the future of my ability to hear. The internet did not have good news. “Flying with an ear infection doesn’t always result in a ruptured eardrum,” one website said. Basically, I had been taking decongestants since midafternoon. I felt like a junkie in a gritty TV show about Times Square in the 1970s, nervous and sweaty and incoherent even as I was beset by an achy, leaden inertia. Soon the lights would go down, part of the airline’s next planned group activity (sleeping) and I would make perhaps the gravest pharmaceutical error of my adult life. But that was still in the future.\n",
      "\n",
      "\n",
      "① More than # feet above the pacific metal.\n",
      "② I on future of my ability to cease..\n",
      "③ Internet does not have good news.\n",
      "④ Emir taking i since last week.\n",
      "⑤ I still in the future.\n"
     ]
    }
   ],
   "source": [
    "# 스포츠 기사 및 연예 기사 활용하여 내용 일치 문제 생성 \n",
    "url = 'https://www.nytimes.com/2019/11/13/travel/qantas-longest-flight.html'\n",
    "raw_txt_name = 'CSAT_27.txt'\n",
    "prepro_txt_name = 'prepro_27.txt'\n",
    "learned_txt_name = 'correct_27.txt'\n",
    "num = 1\n",
    "# 지문의 길이는 뉴스 기사 길이에 따라 유동적으로 설정하기\n",
    "num_words = 400\n",
    "def correct_27(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_correct(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_correct(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    # Rouge Score에 따라 선지 생성 \n",
    "    question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict) # 5개의 선지가 나옴 \n",
    "\n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == num: # 오답 선지 1개 \n",
    "            # 오답으로 변화시키기\n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx != num: # 정답 선지 4개\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사들에 대해서만\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # 유의어 사전을 바탕으로 유사한 의미를 지닌 단어로 변경\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v] # 유의어가 없는 경우가 발생\n",
    "                \n",
    "                if paraphrasing_list == list: # 유의어 사전에서 유사어가 있는 경우\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                elif paraphrasing_list != list: # 유사어가 없는 경우 \n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "        \n",
    "        # 오답 선지 1개, 정답 선지 4개 리스트로 배열\n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    # 넘버링을 포함한 선지로 가공\n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('27. 다음 글에 일치하지 않은 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    correct_27(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 many of these translators got involved because they noticed incorrect or incomplete english transcripts online , but also because they saw an opportunity to participate in the rise of a group they wanted to see succeed\n",
      "1 at the first bts concert the ##-something fan jiye kim (who posts as @doyou_bangtan) attended , she saw the band wanted the concert to be a way for us to share in our joys and pain , just as humans walking alongside each other , she said in a phone interview \n",
      "2 bangtan translations is one of the largest , posting comprehensive , authoritative interpretations of lyrics , tweets and long videos\n",
      "3 the six-person peachboy team does social media posts , lyrics and letters from the subscription-based fancafe platform\n",
      "4 for in-depth looks at lyrics , @doolsetbangtan and @doyou_bangtan offer heavily contextualized , almost academic deep dives\n",
      "5 translating for one of k-pop 's biggest groups comes with pressures\n",
      "6 the sheer amount of content requires some discernment , even as a growing english-speaking market demands more , and wants it faster\n",
      "7 some translators have experienced burnout , especially those working alone at the mercy of an incredibly active and devoted fan base\n",
      "8 the person behind the popular account @cafe_army shared a letter to followers on june ## announcing an indefinite rest to focus on their personal life , which had been compromised by so much time translating\n",
      "9 others can relate: kim averages more than a thousand phone notifications a day\n",
      "10 katie h , who runs @doolsetbangtan , took a short break last year after realizing she felt guilty when she didn’t have time to translate everything\n",
      "11 people think we’re machines , said rachel , whose korean name is yejin , about her work helping run @spotlightbts as a busy ##-year-old college student in the united states\n",
      "12 she said twitter fans will tell her , i don’t know how you balance your life with all of this\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 일치 문제 생성 준비 중.. correct_28.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. correct_28.txt...\n",
      "@는 사전에 없는 단어입니다\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "28. 다음 글에 일치하지 않는 것을 고르시오.\n",
      "-----------------------------------\n",
      "Many of these translators got involved because they noticed incorrect or incomplete English transcripts online, but also because they saw an opportunity to participate in the rise of a group they wanted to see succeed. At the first BTS concert the 20-something fan Jiye Kim (who posts as @doyou_bangtan) attended, she saw the band wanted “the concert to be a way for us to share in our joys and pain, just as humans walking alongside each other,” she said in a phone interview . Bangtan Translations is one of the largest, posting comprehensive, authoritative interpretations of lyrics, tweets and long videos. The six-person Peachboy team does social media posts, lyrics and letters from the subscription-based Fancafe platform. For in-depth looks at lyrics, @doolsetbangtan and @doyou_bangtan offer heavily contextualized, almost academic deep dives. Translating for one of K-pop’s biggest groups comes with pressures. The sheer amount of content requires some discernment, even as a growing English-speaking market demands more, and wants it faster. Some translators have experienced burnout, especially those working alone at the mercy of an incredibly active and devoted fan base. The person behind the popular account @cafe_army shared a letter to followers on June 27 announcing an “indefinite rest” to focus on their personal life, which had been “compromised” by so much time translating. Others can relate: Kim averages more than a thousand phone notifications a day. Katie H., who runs @doolsetbangtan, took a short break last year after realizing she felt “guilty” when she didn’t have time to translate everything. “People think we’re machines,” said Rachel, whose Korean name is Yejin, about her work helping run @SPOTLIGHTBTS as a busy 20-year-old college student in the United States. She said Twitter fans will tell her, “I don’t know how you balance your life with all of this.”\n",
      "\n",
      "\n",
      "① U # s friendship concert to be held in beijing.\n",
      "② Fm spokesman on academic deep emigration.\n",
      "③ @ 's biggest group in.\n",
      "④ @ share letter on common life..\n",
      "⑤ Kim averages 's phone volumes.\n"
     ]
    }
   ],
   "source": [
    "# 내용 일치 문제 생성\n",
    "\n",
    "url = 'https://www.nytimes.com/2019/07/04/arts/music/bts-kpop-translators.html?searchResultPosition=1'\n",
    "raw_txt_name = 'CSAT_28.txt'\n",
    "prepro_txt_name = 'prepro_28.txt'\n",
    "learned_txt_name = 'correct_28.txt'\n",
    "num = 3\n",
    "# 지문의 길이는 뉴스 기사 길이에 따라 유동적으로 설정하기\n",
    "num_words = 300\n",
    "\n",
    "def correct_28(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_correct(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_correct(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    # Rouge Score에 따라 선지 생성 \n",
    "    question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict) # 5개의 선지가 나옴 \n",
    "\n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == num: # 오답 선지 1개 \n",
    "            # 오답ㅇ로 변화시키기\n",
    "            replace_sentence = make_wrong_question(replace_sentence)\n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx != num: # 정답 선지 4개\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사들에 대해서만\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\")]\n",
    "            \n",
    "            # 유의어 사전을 바탕으로 유사한 의미를 지닌 단어로 변경\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v] # 유의어가 없는 경우가 발생\n",
    "                \n",
    "                if paraphrasing_list == list: # 유의어 사전에서 유사어가 있는 경우\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                elif paraphrasing_list != list: # 유사어가 없는 경우\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                \n",
    "            \n",
    "            replace_sentence = ' '.join(token)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "        \n",
    "        # 오답 선지 1개, 정답 선지 4개 리스트로 배열\n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    # 넘버링을 포함한 선지로 가공\n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('28. 다음 글에 일치하지 않는 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    correct_28(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

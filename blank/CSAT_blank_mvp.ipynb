{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jaehyungseo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "from model import Model\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "from rouge import Rouge\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import nltk \n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sym_vocab\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지문 생성 및 문제 생성을 위한 텍스트 파일 생성 \n",
    "def crawl_news(url, raw_txt_name, num_words):\n",
    "    # URL 기반 크롤링, 언어는 영어로 설정\n",
    "    news = Article(url, language = 'en') # URL 가져오기\n",
    "    news.download() # 해당 url의 html을 가져오기\n",
    "    # HTML 파싱 적용\n",
    "    news.parse() \n",
    "    # 지정된 단어 갯수를 기준으로 textrank를 적용하여 파싱된 텍스트를 요약 (Rule base)\n",
    "    text = summarize(news.text, word_count = num_words) # 600자 기준으로 하기\n",
    "    title = news.title # 파싱한 html에서 title에 해당하는 부분 추출하기\n",
    "    title = title.lower() # title에 대해서 전부 소문자화 \n",
    "    \n",
    "    title = re.sub('\\([^)]*\\)', '', title) # 괄호를 포함한 문자열 제거 \n",
    "    \n",
    "    # 빈칸으로 만들 지문을 textrank 기준으로 선별\n",
    "    blank_sentence = summarize(news.text, word_count = 30)\n",
    "    \n",
    "    # 크롤링한 raw data를 별도로 저장\n",
    "    with open(raw_txt_name, 'w', encoding = 'utf-8') as f: # 크롤링한 raw data를 저장하기\n",
    "        f.write(text)\n",
    "\n",
    "    return text, title, blank_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈칸 지문 생성을 위한 함수를 별도로 정의\n",
    "def bf_create_blank(raw_txt_name, prepro_txt_name):\n",
    "    # 문장 번호: 문장 형식의 저장을 위해 변수 생성\n",
    "    test_dict = {}\n",
    "    \n",
    "    # 저장했었던 크롤링 본문 데이터 불러오기\n",
    "    with open(raw_txt_name, 'r', encoding = 'utf-8') as f:\n",
    "    # raw data 상태에서 전처리를 통해 모델에 들어갈 수 있는 데이터셋으로 변형\n",
    "        for idx, text in enumerate(f.readlines()):\n",
    "            # print(idx, text)\n",
    "            text = text.lower() # 소문자로 통일\n",
    "            text = re.sub('[.]', '', text) # 일단 온점 모두 제거\n",
    "            text = re.sub('[,]', ' ,', text) # 반점 하나의 단어로 인식\n",
    "            text = re.sub(\"\\’s\",' \\'s', text) # 소유격도 하나의 단어로 인식\n",
    "            text = re.sub(\"[0-9]\", '#', text) # 숫자를 #로 치환\n",
    "            #text = re.sub(\"'.+'\", '', text) # 작은 따옴표 안 고유명사 제거 \n",
    "            text = re.sub('[“”]', '', text) # 큰 따옴표 제거 \n",
    "            text = re.sub('\\n', '', text) # 줄 바꿈표 제거 \n",
    "            print(idx, text)\n",
    "        \n",
    "            # 사전에 저장하기\n",
    "            test_dict[idx] = text ## {1: '텍스트', 2: '텍스트'}\n",
    "            \n",
    "    # 전처리된 데이터를 별도의 파일로 저장\n",
    "    with open(prepro_txt_name, 'w', encoding = 'utf-8') as f: # 전처리한 데이터를 저장하기 \n",
    "        # 줄 바꿈을 기준으로 문장을 구분하여 저장\n",
    "        for i in range(len(test_dict)):\n",
    "            test_dict[i] = test_dict[i] + \" \" + \".\" + '\\n'\n",
    "            f.write(test_dict[i])\n",
    "    \n",
    "    return test_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 요약 학습 데이터 불러오기\n",
    "def shake_bot_blank(filename, vaild_article_path):\n",
    "    # 설정된 세팅 값 불러오기\n",
    "    with open(\"args.pickle\", \"rb\") as f:\n",
    "        args = pickle.load(f)\n",
    "        \n",
    "    # 실행시 최초의 상태로 초기화하여 모델 재사용이 가능하도록 함\n",
    "    tf.reset_default_graph()\n",
    "    # 학습된 단어 사전과 본문 최대 길이와 요약 최대 길이 설정 값을 불러옴\n",
    "    print(\"셰봇이 단어 사전 불러오는 중...\")\n",
    "    word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "    # 불러온 설정 값에 따라서 검증 데이터셋을 생성\n",
    "    print(\"셰봇이 기본 설정 값 초기화 중...\")\n",
    "    valid_x = build_dataset(\"valid\", vaild_article_path, word_dict, article_max_len, summary_max_len, args.toy)\n",
    "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # 학습된 체크포인트와 모델을 불러오기 \n",
    "        print(\"Loading saved model...\")\n",
    "        model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True) # 검증 단계\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # 검증을 진행할 배치 사이즈 설정\n",
    "        batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "        \n",
    "        # 검증 단계인 만큼 인코더 인풋 부분만 고려하여 학습 진행\n",
    "        print(\"셰봇이 수능 빈칸 문제 생성 준비 중.. {}...\".format(filename))\n",
    "        for batch_x, _ in batches:\n",
    "            # 문장 -> 문장 예측이 가능하도록 데이터 분할 \n",
    "            batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "            # 검증을 위한 배치에 따라 모델 및 데이터 셋 설정 준비 \n",
    "            valid_feed_dict = {\n",
    "                model.batch_size: len(batch_x),\n",
    "                model.X: batch_x,\n",
    "                model.X_len: batch_x_len,\n",
    "            }\n",
    "            # Prediction을 목적으로 학습된 모델을 검증을 위한 설정 값을 적용하여 예측 결과 생성\n",
    "            prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "            # 인덱스 -> 단어 사전을 통해 학습된 결과를 단어로 표현.\n",
    "            prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "            \n",
    "            # 생성된 예측 결과물을 문장 단위로 저장 \n",
    "            with open(filename, \"w\") as f:\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    print(\" \".join(summary), file=f)\n",
    "                    \n",
    "            #with open('./rouge_test/sys_dir/{}.txt'.format(filename), \"w\") as f:\n",
    "                #for line in prediction_output:\n",
    "                    #summary = list()\n",
    "                    #for word in line:\n",
    "                        #if word == \"</s>\":\n",
    "                            #break\n",
    "                        #if word not in summary:\n",
    "                            #summary.append(word)\n",
    "                    #print(\" \".join(summary), file=f)\n",
    "\n",
    "        print('셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. {}...'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## textrank 없이 rouge score로 학습 완성도가 높은 문장을 선별 \n",
    "def for_rouge_test(summy, label):\n",
    "    \n",
    "    rouge_dict = {}\n",
    "    rouge_list = []\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # 학습 결과로 예측한 결과물을 불러오기 \n",
    "    with open(summy, 'r', encoding = 'utf-8') as f:\n",
    "        \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            # 실제 문장 - 예측 결과에서 학습의 정도를 Rouge-r과 Rouge-p로 검증\n",
    "            scores = rouge.get_scores(line, label[idx])\n",
    "            \n",
    "            p_score = scores[0][\"rouge-1\"][\"p\"]\n",
    "            r_score = scores[0][\"rouge-1\"][\"r\"]\n",
    "            \n",
    "            r_idx_score = (idx, r_score)\n",
    "            p_idx_score = (idx, p_score)\n",
    "            \n",
    "            # Rouge-r 사용\n",
    "            rouge_list.append(r_idx_score)\n",
    "            #rouge_list.append(p_idx_score)\n",
    "            \n",
    "            rouge_dict[idx] = line \n",
    "        \n",
    "        # 내림차순으로 가장 점수가 높은 문장을 순서로 리스트 배열\n",
    "        sort_rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[-1], reverse=True)\n",
    "        # 상위 5개 문장 추출하기 \n",
    "        sort_rouge_list = sort_rouge_list[:5]\n",
    "        \n",
    "        question_list = []\n",
    "    \n",
    "        \n",
    "        for ix in sort_rouge_list:\n",
    "            \n",
    "            question_list.append(rouge_dict[ix[0]])\n",
    "        \n",
    "    # 상위 5개 문장에 대해서 선지로 활용할 수 있도록 리턴  \n",
    "    return question_list, sort_rouge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## textrank로 중요도가 높은 문장을 사전 추출하고 제작\n",
    "def for_rouge_test2(summy, label, blank_sentence):\n",
    "    rouge_dict = {}\n",
    "    rouge_list = []\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    # 학습 결과로 예측한 결과물을 불러오기 \n",
    "    with open(summy, 'r', encoding = 'utf-8') as f:\n",
    "            \n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            \n",
    "            # 실제 문장 - 빈칸 지문에서 학습의 정도를 Rouge-r과 Rouge-p로 검증\n",
    "            scores = rouge.get_scores(label[idx], blank_sentence)\n",
    "            \n",
    "            print(blank_sentence)\n",
    "            \n",
    "            p_score = scores[0][\"rouge-1\"][\"p\"] \n",
    "            r_score = scores[0][\"rouge-1\"][\"r\"] \n",
    "            \n",
    "            r_idx_score = (idx, r_score)\n",
    "            p_idx_score = (idx, p_score)\n",
    "            \n",
    "            # Rouge-r score 사용\n",
    "            rouge_list.append(r_idx_score)\n",
    "            #rouge_list.append(p_idx_score)\n",
    "            #print(rouge_list)\n",
    "            \n",
    "            rouge_dict[idx] = line\n",
    "        \n",
    "        # 내림차순으로 가장 점수가 높은 문장을 순서로 리스트 배열\n",
    "        sort_rouge_list = sorted(rouge_list, key = lambda rouge_list: rouge_list[-1], reverse=True)\n",
    "        # 상위 5개 문장 추출하기\n",
    "        sort_rouge_list = sort_rouge_list[:5]\n",
    "        \n",
    "        \n",
    "        question_list = []\n",
    "    \n",
    "        \n",
    "        for ix in sort_rouge_list:\n",
    "            \n",
    "            question_list.append(rouge_dict[ix[0]])\n",
    "        \n",
    "    # 상위 5개 문장에 대해서 선지로 활용할 수 있도록 리턴\n",
    "    return question_list, sort_rouge_list\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과물 중 <unk> 부분 처리\n",
    "def delete_unk(content, unk_sentence):\n",
    "    \n",
    "    container = []\n",
    "    \n",
    "    text = open(content, 'r', encoding = 'utf-8').read()\n",
    "    \n",
    "    # 각 단어에 대해서 품사 태깅\n",
    "    tagged_list = pos_tag(word_tokenize(text))\n",
    "    \n",
    "    # 명사 부분만 활용\n",
    "    nnp_list = [t[0] for t in tagged_list if (t[1] == \"NNP\") or (t[1] == \"NN\")]\n",
    "    \n",
    "    fd_names = FreqDist(nnp_list)\n",
    "    \n",
    "    # 빈도수 기반으로 Unk 처리하기\n",
    "    freq = fd_names.most_common(2)\n",
    "    \n",
    "    #for tagged in tagged_list:\n",
    "        \n",
    "        #if tagged[1] == 'NN' or 'NNP':\n",
    "            \n",
    "            #container.append(tagged[0]) # 단어만 추출\n",
    "    \n",
    "    \n",
    "    # unk 부분을 처리하는 방법 중 하나로 최고 빈도수를 기반으로 하는 방식\n",
    "    ## 현재 여기서는 정확도를 해치는 것으로 판단 \n",
    "    #replace_sentence = re.sub('< unk >', freq[0][0], unk_sentence)\n",
    "    \n",
    "    # 다중 숫자에 대한 표현은 Several로 대체 \n",
    "    replace_sentence = re.sub('# nd', 'Several', unk_sentence)\n",
    "    \n",
    "    #replace_sentence = re.sub('# nd', 'Several', replace_sentence)\n",
    "    \n",
    "    # 불필요한 줄 바꿈표 제거 \n",
    "    replace_sentence = re.sub('\\n', '', replace_sentence)\n",
    "    \n",
    "    return replace_sentence\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문에 빈칸 만들기\n",
    "def make_blank(idx, text): # 1개만 넣어야 함.\n",
    "    # 줄바꿈 단위로 문장을 나누기 \n",
    "    text_split = text.split('\\n')\n",
    "    print(text_split)\n",
    "    # 선정한 문장 <-> 빈칸 교체\n",
    "    text_split[idx] = '_______________________________.'\n",
    "    text = '\\n'.join(text_split)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문에 동사 이후 빈칸 만들기 (S + V + O 라면, O 이후)\n",
    "def make_blank_verb(idx, text, blank_sent): # 1개만 넣어야 함.\n",
    "    # 줄바꿈 단위로 문장을 나누기 \n",
    "    text_split = text.split('\\n')\n",
    "    print(text_split)\n",
    "    text_split[idx] = blank_sent + ' _______________________________.'\n",
    "    text = '\\n'.join(text_split)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문에 주어에 해당하는 명사 이후 빈칸 만들기 \n",
    "def make_blank_noun(idx, text, blank_sent): # 1개만 넣어야 함.\n",
    "    # 줄바꿈 단위로 문장을 나누기 \n",
    "    text_split = text.split('\\n')\n",
    "    print(text_split)\n",
    "    text_split[idx] = blank_sent + ' _______________________________.'\n",
    "    text = '\\n'.join(text_split)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "0 sydney , australia — thousands of australians have taken to the streets this week to protest the police killing of an aboriginal teenager in a remote community , a case that has added fuel to long-simmering anger over the government 's behavior toward the country 's indigenous people\n",
      "1 on wednesday , the police officer who fatally shot the ##-year-old man , kumanjayi walker , in the central australian town of yuendumu was charged with murder\n",
      "2 but questions have continued to swirl about what happened when officers were alone with mr walker , as well as how he and his family were treated in the hours that followed\n",
      "3 mr walker did not receive medical care after he was shot on saturday , because staff had evacuated the clinic in yuendumu earlier in the day over safety concerns\n",
      "4 that left any medical emergencies to be handled by another clinic nearly ## miles away\n",
      "5 and after mr walker died , family members who waited outside a police station were not informed about his death until ## hours later\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:19: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:20: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:22: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:34: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:35: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/Desktop/text-summarization-tensorflow-master/blank/model.py:40: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jaehyungseo/anaconda3/envs/jaehyung37/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/beam_search_decoder.py:971: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 빈칸 문제 생성 준비 중.. blank_31.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. blank_31.txt...\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.\n",
      "['< unk > hospital closes in\\n', '< unk > walker charged with murder\\n', 'australians protest police killing of aboriginal teenager\\n', 'sri lankan police learn from mr # walker\\n', 'medical services to be held in macao\\n']\n",
      "< unk > walker charged with murder.\n",
      "Australians protest police killing of aboriginal teenager.\n",
      "Sri lankan police learn from mr # walker.\n",
      "Medical services to be held in macao.\n",
      "['SYDNEY, Australia — Thousands of Australians have taken to the streets this week to protest the police killing of an Aboriginal teenager in a remote community, a case that has added fuel to long-simmering anger over the government’s behavior toward the country’s Indigenous people.', 'On Wednesday, the police officer who fatally shot the 19-year-old man, Kumanjayi Walker, in the central Australian town of Yuendumu was charged with murder.', 'But questions have continued to swirl about what happened when officers were alone with Mr. Walker, as well as how he and his family were treated in the hours that followed.', 'Mr. Walker did not receive medical care after he was shot on Saturday, because staff had evacuated the clinic in Yuendumu earlier in the day over safety concerns.', 'That left any medical emergencies to be handled by another clinic nearly 40 miles away.', 'And after Mr. Walker died, family members who waited outside a police station were not informed about his death until 10 hours later.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "31. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.\n",
      "-----------------------------------\n",
      "SYDNEY, Australia — Thousands of Australians have taken to the streets this week to protest the police killing of an Aboriginal teenager in a remote community, a case that has added fuel to long-simmering anger over the government’s behavior toward the country’s Indigenous people. On Wednesday, the police officer who fatally shot the 19-year-old man, Kumanjayi Walker, in the central Australian town of Yuendumu was charged with murder. But questions have continued to swirl about what happened when officers were alone with Mr. Walker, as well as how he and his family were treated in the hours that followed. _______________________________. That left any medical emergencies to be handled by another clinic nearly 40 miles away. And after Mr. Walker died, family members who waited outside a police station were not informed about his death until 10 hours later.\n",
      "\n",
      "\n",
      "① < unk > walker charged with murder.\n",
      "② Medical services to be held in macao.\n",
      "③ Sri lankan police learn from mr # walker.\n",
      "④ < unk > hospital closes in.\n",
      "⑤ Australians protest police killing of aboriginal teenager.\n"
     ]
    }
   ],
   "source": [
    "# 문장 빈칸 문제 생성\n",
    "url = 'https://www.nytimes.com/2019/11/14/world/australia/police-shooting-murder-Indigenous.html'\n",
    "raw_txt_name = 'CSAT_31.txt'\n",
    "prepro_txt_name = 'prepro_31.txt'\n",
    "learned_txt_name = 'blank_31.txt'\n",
    "num_words = 400\n",
    "\n",
    "def blank_31(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = False):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title, blank_sentence = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    \n",
    "    print(blank_sentence)\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_blank(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_blank(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # Textrank를 적용한 Rouge Score에 따라 선지 생성 \n",
    "    if textrank == True:\n",
    "    \n",
    "        question_list, sort_rouge_list = for_rouge_test2(learned_txt_name, test_dict, blank_sentence)\n",
    "    \n",
    "        print(question_list)\n",
    "    \n",
    "    # Textrank를 적용하지 않은 Rouge Score에 따라 선지 생성\n",
    "    if textrank == False:\n",
    "        \n",
    "        question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == 0: # 첫 번째는 항상 빈칸 지문임. \n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx > 0: # 나머지는 다른 지엽정 정보를 요약한 형태 \n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사, 명사, 형용사에서 유사어로 치환\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\") or\n",
    "                     (t[1] == \"NN\") or (t[1] == \"NNS\") or (t[1] == \"NNP\") or (t[1] == \"NNPS\") or (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\") or (t[1] == \"RB\")]\n",
    "            \n",
    "            # 유사어 치환\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                # 유사어가 존재하는 경우 랜덤하게 1가지 추출\n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                # 유사어가 존재하지 않는 경우 최초 입력값 그대로 다시 반환\n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                    \n",
    "            replace_sentence = ' '.join(token)\n",
    "            \n",
    "            #replace_sentence = re.sub(\" \\’s\",'\\'s', replace_sentence)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "            \n",
    "            print(replace_sentence)\n",
    "        \n",
    "        # 빈칸 선지 1개, 지엽적 선지 4개 저장\n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    # 빈칸으로 설정한 인덱스 추적 -> 해당 문장 번호를 빈칸으로 대체 \n",
    "    text = make_blank(sort_rouge_list[0][0], text)\n",
    "        \n",
    "    text = re.sub('\\n', ' ', text) # raw data에서 불필요한 줄바꿈표 제거 (상대적일듯)\n",
    "    \n",
    "    # 정답 빈칸이 무조건 1번이 였으므로, 랜덤하게 섞어주기\n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        # 랜덤하게 섞은 선지에 넘버링 \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('31. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    blank_31(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Instagram, photographs purporting to show the whistle-blower with a number of public figures from the Democratic Party were also widely shared.\n",
      "0 youtube had taken additional steps to make it difficult to search for the name , removing an autocomplete feature that filled in the name once a person began typing it\n",
      "1 on tuesday morning , the top three videos claiming to share the whistle-blower 's name each had over ### ,### views\n",
      "2 instead , they shared it in the comments and discussions below the video , where people also linked to blogs discussing the identity\n",
      "3 many videos with the name were taken down after the new york times asked youtube about them\n",
      "4 facebook was unwilling to comment on whether it was seeing a coordinated effort to spread the whistle-blower 's name , but said it would continue to take down content that violated its policies\n",
      "5 youtube did not respond to a request for comment on the videos being shared on its site\n",
      "6 on instagram , photographs purporting to show the whistle-blower with a number of public figures from the democratic party were also widely shared\n",
      "7 earlier in the week , instagram had blocked searches for the last name being circulated , or for the hashtag #whistle-blower\n",
      "8 but on thursday , it was possible to search on that hashtag , as well as the purported full name or variations of the name\n",
      "9 instagram said it temporarily blocked searches for #whistle-blower while it calibrated its systems to ensure that legitimate searches for the term , or the purported name , were not being affected\n",
      "10 a twitter spokeswoman said posting names and images did not violate twitter rules\n",
      "11 she said twitter bans only specific types of private information , such as sharing a person 's home address , private phone number or government identification without their consent\n",
      "12 president trump , and his supporters , have called for the name of the whistle-blower to be made public\n",
      "13 on wednesday , during the house 's first public hearing in its impeachment inquiry , republican members of congress repeated calls for the whistle-blower to be identified\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 빈칸 문제 생성 준비 중.. blank_32.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. blank_32.txt...\n",
      "New york times takes on youtube.\n",
      "President asks for naming of mexican whistle-blower.\n",
      "Top u # s whistle-blower 's three.\n",
      "China bans use of private information.\n",
      "['YouTube had taken additional steps to make it difficult to search for the name, removing an autocomplete feature that filled in the name once a person began typing it.', 'On Tuesday morning, the top three videos claiming to share the whistle-blower’s name each had over 100,000 views.', 'Instead, they shared it in the comments and discussions below the video, where people also linked to blogs discussing the identity.', 'Many videos with the name were taken down after The New York Times asked YouTube about them.', 'Facebook was unwilling to comment on whether it was seeing a coordinated effort to spread the whistle-blower’s name, but said it would continue to take down content that violated its policies.', 'YouTube did not respond to a request for comment on the videos being shared on its site.', 'On Instagram, photographs purporting to show the whistle-blower with a number of public figures from the Democratic Party were also widely shared.', 'Earlier in the week, Instagram had blocked searches for the last name being circulated, or for the hashtag #whistle-blower.', 'But on Thursday, it was possible to search on that hashtag, as well as the purported full name or variations of the name.', 'Instagram said it temporarily blocked searches for #whistle-blower while it calibrated its systems to ensure that legitimate searches for the term, or the purported name, were not being affected.', 'A Twitter spokeswoman said posting names and images did not violate Twitter rules.', 'She said Twitter bans only specific types of private information, such as sharing a person’s home address, private phone number or government identification without their consent.', 'President Trump, and his supporters, have called for the name of the whistle-blower to be made public.', 'On Wednesday, during the House’s first public hearing in its impeachment inquiry, Republican members of Congress repeated calls for the whistle-blower to be identified.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "32. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.\n",
      "-----------------------------------\n",
      "YouTube had taken additional steps to make it difficult to search for the name, removing an autocomplete feature that filled in the name once a person began typing it. On Tuesday morning, the top three videos claiming to share the whistle-blower’s name each had over 100,000 views. Instead, they shared it in the comments and discussions below the video, where people also linked to blogs discussing the identity. Many videos with the name were taken down after The New York Times asked YouTube about them. Facebook was unwilling to comment on whether it was seeing a coordinated effort to spread the whistle-blower’s name, but said it would continue to take down content that violated its policies. _______________________________. On Instagram, photographs purporting to show the whistle-blower with a number of public figures from the Democratic Party were also widely shared. Earlier in the week, Instagram had blocked searches for the last name being circulated, or for the hashtag #whistle-blower. But on Thursday, it was possible to search on that hashtag, as well as the purported full name or variations of the name. Instagram said it temporarily blocked searches for #whistle-blower while it calibrated its systems to ensure that legitimate searches for the term, or the purported name, were not being affected. A Twitter spokeswoman said posting names and images did not violate Twitter rules. She said Twitter bans only specific types of private information, such as sharing a person’s home address, private phone number or government identification without their consent. President Trump, and his supporters, have called for the name of the whistle-blower to be made public. On Wednesday, during the House’s first public hearing in its impeachment inquiry, Republican members of Congress repeated calls for the whistle-blower to be identified.\n",
      "\n",
      "\n",
      "① China bans use of private information.\n",
      "② New york times takes on youtube.\n",
      "③ Top u # s whistle-blower 's three.\n",
      "④ Youtube unable to comment on videos.\n",
      "⑤ President asks for naming of mexican whistle-blower.\n"
     ]
    }
   ],
   "source": [
    "# 문장 빈칸 문제 생성\n",
    "url = 'https://www.nytimes.com/2019/11/14/technology/whistleblower-name-facebook-youtube.html?action=click&module=Top%20Stories&pgtype=Homepage'\n",
    "raw_txt_name = 'CSAT_32.txt'\n",
    "prepro_txt_name = 'prepro_32.txt'\n",
    "learned_txt_name = 'blank_32.txt'\n",
    "num_words = 300\n",
    "\n",
    "def blank_32(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = False):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title, blank_sentence = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    \n",
    "    print(blank_sentence)\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_blank(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_blank(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # Textrank를 적용한 Rouge Score에 따라 선지 생성 \n",
    "    if textrank == True:\n",
    "    \n",
    "        question_list, sort_rouge_list = for_rouge_test2(learned_txt_name, test_dict, blank_sentence)\n",
    "    \n",
    "        print(question_list)\n",
    "    \n",
    "    # Textrank를 적용하지 않은 Rouge Score에 따라 선지 생성\n",
    "    if textrank == False:\n",
    "        \n",
    "        question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "        \n",
    "        if idx == 0: # 첫 번째는 항상 빈칸 지문임. \n",
    "            \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"   \n",
    "    \n",
    "        if idx > 0: # 나머지는 다른 지엽정 정보를 요약한 형태 \n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사, 명사, 형용사에서 유사어로 치환\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\") or\n",
    "                     (t[1] == \"NN\") or (t[1] == \"NNS\") or (t[1] == \"NNP\") or (t[1] == \"NNPS\") or (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\") or (t[1] == \"RB\")]\n",
    "            \n",
    "            # 유사어 치환\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                # 유사어가 존재하는 경우 랜덤하게 1가지 추출\n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                # 유사어가 존재하지 않는 경우 최초 입력값 그대로 다시 반환\n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                    \n",
    "            replace_sentence = ' '.join(token)\n",
    "            \n",
    "            #replace_sentence = re.sub(\" \\’s\",'\\'s', replace_sentence)\n",
    "                \n",
    "            replace_sentence = replace_sentence.capitalize() + \".\"\n",
    "            \n",
    "            print(replace_sentence)\n",
    "        \n",
    "        # 빈칸 선지 1개, 지엽적 선지 4개 저장\n",
    "        question_list[idx] = replace_sentence\n",
    "       \n",
    "    # 빈칸으로 설정한 인덱스 추적 -> 해당 문장 번호를 빈칸으로 대체 \n",
    "    text = make_blank(sort_rouge_list[0][0], text)\n",
    "        \n",
    "    text = re.sub('\\n', ' ', text) # raw data에서 불필요한 줄바꿈표 제거 (상대적일듯)\n",
    "    \n",
    "    # 정답 빈칸이 무조건 1번이 였으므로, 랜덤하게 섞어주기\n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        # 랜덤하게 섞은 선지에 넘버링 \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('32. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    blank_32(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 african-americans benefited regardless of fish intake , showing a ## percent lower risk of heart attack\n",
      "1 this could be a chance finding , said dr joann manson , a director of the study and the chief of the division of preventive medicine at brigham and women 's hospital\n",
      "2 we do plan to pursue it in greater detail and try to replicate it in a separate trial because if this can be reproduced , that would be a very dramatic benefit to african-americans\n",
      "3 because there is still more research to be done , experts don’t necessarily recommended that african-americans take omega-#\n",
      "4 if you have some history of heart disease or high triglycerides ( an estimated ## percent of adults in the united states do , according to data from the national health and nutrition examination survey in #### ) , it may be a good idea to take omega-#\n",
      "5 the potential downside , because supplements are not regulated , is that production isn’t standardized so we don’t know what 's in them , according to dr pieter cohen , of cambridge health alliance , who is an associate professor of medicine at harvard medical school\n",
      "6 he said that supplements are expensive and that money could alternatively be spent on a healthier diet\n",
      "7 as an internist , dr cohen has seen negative behavioral effects in some of his patients who take supplements\n",
      "8 i have many patients who are like , ‘i’ll take my supplement and then i won’t worry about eating healthfully during the day ,’ dr cohen said\n",
      "9 that 's really misguided\n",
      "10 because in this case we have absolutely no evidence that replacing a healthy meal of fish with an omega-# supplement is better\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 빈칸 문제 생성 준비 중.. blank_33.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. blank_33.txt...\n",
      "['African-Americans benefited regardless of fish intake, showing a 77 percent lower risk of heart attack.', '“This could be a chance finding,” said Dr. JoAnn Manson, a director of the study and the chief of the Division of Preventive Medicine at Brigham and Women’s Hospital.', '“We do plan to pursue it in greater detail and try to replicate it in a separate trial because if this can be reproduced, that would be a very dramatic benefit to African-Americans.”', 'Because there is still more research to be done, experts don’t necessarily recommended that African-Americans take omega-3.', 'If you have some history of heart disease or high triglycerides ( an estimated 25 percent of adults in the United States do, according to data from the National Health and Nutrition Examination Survey in 2015 ), it may be a good idea to take omega-3.', 'The potential downside, because supplements are not regulated, is that production isn’t standardized so we don’t know what’s in them, according to Dr. Pieter Cohen, of Cambridge Health Alliance, who is an associate professor of medicine at Harvard Medical School.', 'He said that supplements are expensive and that money could alternatively be spent on a healthier diet.', 'As an internist, Dr. Cohen has seen negative behavioral effects in some of his patients who take supplements.', '“I have many patients who are like, ‘I’ll take my supplement and then I won’t worry about eating healthfully during the day,’” Dr. Cohen said.', '“That’s really misguided.', 'Because in this case we have absolutely no evidence that replacing a healthy meal of fish with an omega-3 supplement is better.”']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "33. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.\n",
      "-----------------------------------\n",
      "African-Americans benefited regardless of fish intake, showing a 77 percent lower risk of heart attack. “This could be a chance finding,” said Dr. JoAnn Manson, a director of the study and the chief of the Division of Preventive Medicine at Brigham and Women’s Hospital. “We do plan to pursue it in greater detail and try to replicate it in a separate trial because if this can be reproduced, that would be a very dramatic benefit to African-Americans.” Because there is still more research to be done, experts don’t necessarily recommended that African-Americans take omega-3. If you have some history of heart disease or high triglycerides ( an estimated 25 percent of adults in the United States do, according to data from the National Health and Nutrition Examination Survey in 2015 ), it may be a good idea to take omega-3. The potential downside, because supplements are not regulated, is that production isn’t standardized so we don’t know what’s in them, according to Dr. Pieter Cohen, of Cambridge Health Alliance, who is an associate professor of medicine at Harvard Medical School. < unk > diet to be _______________________________. As an internist, Dr. Cohen has seen negative behavioral effects in some of his patients who take supplements. “I have many patients who are like, ‘I’ll take my supplement and then I won’t worry about eating healthfully during the day,’” Dr. Cohen said. “That’s really misguided. Because in this case we have absolutely no evidence that replacing a healthy meal of fish with an omega-3 supplement is better.”\n",
      "\n",
      "\n",
      "① african-americans benefit from fish intake\n",
      "② misguided\n",
      "③ spent on healthier\n",
      "④ to be done\n",
      "⑤ a chance to\n"
     ]
    }
   ],
   "source": [
    "# 동사 이후 빈칸 만들기 \n",
    "url = 'https://www.nytimes.com/2019/11/01/style/self-care/fish-oil-benefits.html'\n",
    "raw_txt_name = 'CSAT_33.txt'\n",
    "prepro_txt_name = 'prepro_33.txt'\n",
    "learned_txt_name = 'blank_33.txt'\n",
    "num_words = 500\n",
    "\n",
    "def blank_33(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = False):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인 추출하기\n",
    "    text, title, blank_sentence = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    \n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_blank(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_blank(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # Textrank를 적용한 Rouge Score에 따라 선지 생성 \n",
    "    if textrank == True:\n",
    "        \n",
    "        question_list, sort_rouge_list = for_rouge_test2(learned_txt_name, test_dict, blank_sentence)\n",
    "    \n",
    "    # Textrank를 적용하지 않은 Rouge Score에 따라 선지 생성\n",
    "    if textrank == False:\n",
    "        \n",
    "        question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "            \n",
    "            # ans_sent = replace_sentence # 동사 빈칸 문제 생성을 위해 정답 선지 사전 대입 \n",
    "        \n",
    "        # 정답 선지를 제외한 선지에 대해 전처리 적용 (이 부분도 재검토)\n",
    "        if idx > 0:\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사, 명사, 형용사에서 유사어로 치환\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\") or\n",
    "                     (t[1] == \"NN\") or (t[1] == \"NNS\") or (t[1] == \"NNP\") or (t[1] == \"NNPS\") or (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\") or (t[1] == \"RB\")]\n",
    "            \n",
    "            # 유사어 치환\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                # 유사어가 존재하는 경우 랜덤하게 1가지 추출\n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                # 유사어가 존재하지 않는 경우 최초 입력값 그대로 다시 반환\n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                    \n",
    "            replace_sentence = ' '.join(token)\n",
    "            \n",
    "        # 빈칸 선지 1개, 지엽적 선지 4개 저장\n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    \n",
    "    for i, q in enumerate(question_list): # 선지 뽑아내기 \n",
    "    \n",
    "        token_q = word_tokenize(q)\n",
    "    \n",
    "        tag_q = pos_tag(token_q)\n",
    "        \n",
    "        \n",
    "        for ix, t in enumerate(tag_q):\n",
    "            # 동사 유무 판단, 동사 위치를 인덱스 값으로 저장\n",
    "            if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\"):\n",
    "                \n",
    "                idx_num = ix\n",
    "                \n",
    "                break\n",
    "            \n",
    "        # 해당 문장에 동사가 이상 없이 존재하여, 문장에서 최초의 동사를 찾는다면\n",
    "        if ix == len(tag_q) - 1:\n",
    "            # 선지 리스트에 넣기\n",
    "            question_list[i] = q \n",
    "            \n",
    "            break\n",
    "                \n",
    "        # 첫번째 원소의 경우 빈칸 정답으로 활용. \n",
    "        elif i == 0: \n",
    "            \n",
    "            save_sent = token_q[:idx_num + 1] # 동사 이전 문장까지 살리기\n",
    "            \n",
    "            save_sent = ' '.join(save_sent)\n",
    "            \n",
    "            save_sent = save_sent.capitalize()\n",
    "            \n",
    "            abandon_sent = token_q[idx_num + 1:] # 동사 이후 문장 선지화 \n",
    "            \n",
    "            abandon_sent = ' '.join(abandon_sent) \n",
    "            \n",
    "            question_list[i] = abandon_sent # 동사 이후 문장은 선지에 넣기 \n",
    "            \n",
    "            \n",
    "        # 나머지 4개 원소의 경우 선지로 활용\n",
    "        else:\n",
    "            \n",
    "            abandon_sent = token_q[idx_num + 1:] # 동사 이후 문장 선지화 \n",
    "            \n",
    "            abandon_sent = ' '.join(abandon_sent) \n",
    "        \n",
    "            question_list[i] = abandon_sent # 동사 이후 문장은 선지에 넣기 \n",
    "            \n",
    "    # 빈칸으로 설정한 인덱스 추적 -> 해당 문장 번호를 빈칸으로 대체 \n",
    "    text = make_blank_verb(sort_rouge_list[0][0], text, blank_sent = save_sent)\n",
    "        \n",
    "    text = re.sub('\\n', ' ', text) # raw data에서 불필요한 줄바꿈표 제거 (상대적일듯)\n",
    "\n",
    "        \n",
    "        \n",
    "    # 정답 빈칸이 현재 무조건 1번이므로, 랜덤하게 섞어주기 \n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('33. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    blank_33(url, raw_txt_name, prepro_txt_name, learned_txt_name,num_words, textrank = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 last week  , a unanimous three-judge panel of a federal appeals court in manhattan ruled against mr trump\n",
      "1 the court , in a focused ruling , said state prosecutors may require third parties to turn over a sitting president 's financial records for use in a grand jury investigation \n",
      "2 mr trump has fought vigorously to shield his financial records , and prosecutors in manhattan have agreed not to seek the tax returns until the case is resolved by the supreme court\n",
      "3 in exchange , they insisted on a very quick briefing schedule , one that would allow the court to announce whether it would hear the case as soon as next month and to issue a decision by june , as the presidential election enters its final stages\n",
      "4 other cases involving mr trump are also in the pipeline\n",
      "5 they involve matters as diverse as demands from house democrats for tax and business records , a request for access to redacted portions of the report prepared by robert s\n",
      "6 mueller iii , the special counsel , and challenges to mr trump 's business arrangements under the constitution 's emoluments clauses\n",
      "7 on wednesday , the full united states court of appeals for the district of columbia circuit refused to rehear a ruling from a divided three-judge panel that mr trump 's accounting firm must comply with the house oversight and reform committee 's demands for eight years of his financial records\n",
      "8 a lawyer for mr trump said he would appeal that ruling to the supreme court , too\n",
      "9 the legal fight in the new york case began in late august after prosecutors in the office of the manhattan district attorney , cyrus r\n",
      "10 vance jr\n",
      "11  , a democrat , subpoenaed mr trump 's accounting firm , mazars usa  , for his tax returns and those of his family business dating to ####\n",
      "12 in thursday 's filing , mr trump 's lawyers called that move unprecedented\n",
      "13 for the first time in our nation 's history , they wrote , a state or local prosecutor has launched a criminal investigation of the president of the united states and subjected him to coercive criminal process\n",
      "14 the petition said the subpoena was politically motivated and sought information that was not relevant to any legitimate criminal inquiry\n",
      "셰봇이 단어 사전 불러오는 중...\n",
      "셰봇이 기본 설정 값 초기화 중...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "셰봇이 수능 빈칸 문제 생성 준비 중.. blank_34.txt...\n",
      "셰봇이가 5지 선다로 쓸만한 친구들을 추려냈어요.. blank_34.txt...\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.\n",
      "['Last week , a unanimous three-judge panel of a federal appeals court in Manhattan ruled against Mr. Trump.', 'The court, in a focused ruling, said state prosecutors may require third parties to turn over a sitting president’s financial records for use in a grand jury investigation .', 'Mr. Trump has fought vigorously to shield his financial records, and prosecutors in Manhattan have agreed not to seek the tax returns until the case is resolved by the Supreme Court.', 'In exchange, they insisted on a very quick briefing schedule, one that would allow the court to announce whether it would hear the case as soon as next month and to issue a decision by June, as the presidential election enters its final stages.', 'Other cases involving Mr. Trump are also in the pipeline.', 'They involve matters as diverse as demands from House Democrats for tax and business records, a request for access to redacted portions of the report prepared by Robert S.', 'Mueller III, the special counsel, and challenges to Mr. Trump’s business arrangements under the Constitution’s emoluments clauses.', 'On Wednesday, the full United States Court of Appeals for the District of Columbia Circuit refused to rehear a ruling from a divided three-judge panel that Mr. Trump’s accounting firm must comply with the House Oversight and Reform Committee’s demands for eight years of his financial records.', 'A lawyer for Mr. Trump said he would appeal that ruling to the Supreme Court, too.', 'The legal fight in the New York case began in late August after prosecutors in the office of the Manhattan district attorney, Cyrus R.', 'Vance Jr.', ', a Democrat, subpoenaed Mr. Trump’s accounting firm, Mazars USA , for his tax returns and those of his family business dating to 2011.', 'In Thursday’s filing, Mr. Trump’s lawyers called that move unprecedented.', '“For the first time in our nation’s history,” they wrote, “a state or local prosecutor has launched a criminal investigation of the president of the United States and subjected him to coercive criminal process.”', 'The petition said the subpoena was “politically motivated” and sought information that was not relevant to any legitimate criminal inquiry.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "34. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.\n",
      "-----------------------------------\n",
      "Last week , a unanimous three-judge panel of a federal appeals court in Manhattan ruled against Mr. Trump. The court, in a focused ruling, said state prosecutors may require third parties to turn over a sitting president’s financial records for use in a grand jury investigation . Mr. Trump has fought vigorously to shield his financial records, and prosecutors in Manhattan have agreed not to seek the tax returns until the case is resolved by the Supreme Court. In exchange, they insisted on a very quick briefing schedule, one that would allow the court to announce whether it would hear the case as soon as next month and to issue a decision by June, as the presidential election enters its final stages. Other cases involving Mr. Trump are also in the pipeline. They involve matters as diverse as demands from House Democrats for tax and business records, a request for access to redacted portions of the report prepared by Robert S. Mueller III, the special counsel, and challenges to Mr. Trump’s business arrangements under the Constitution’s emoluments clauses. U # s court _______________________________. A lawyer for Mr. Trump said he would appeal that ruling to the Supreme Court, too. The legal fight in the New York case began in late August after prosecutors in the office of the Manhattan district attorney, Cyrus R. Vance Jr. , a Democrat, subpoenaed Mr. Trump’s accounting firm, Mazars USA , for his tax returns and those of his family business dating to 2011. In Thursday’s filing, Mr. Trump’s lawyers called that move unprecedented. “For the first time in our nation’s history,” they wrote, “a state or local prosecutor has launched a criminal investigation of the president of the United States and subjected him to coercive criminal process.” The petition said the subpoena was “politically motivated” and sought information that was not relevant to any legitimate criminal inquiry.\n",
      "\n",
      "\n",
      "① refuses to rehear landmark ruling\n",
      "② prosecutors agree not to seek tax\n",
      "③ < unk > usa\n",
      "④ parties urged to turn over sitting president 's\n",
      "⑤ > request for access to white house\n"
     ]
    }
   ],
   "source": [
    "# 주어 이후 서술형 빈칸 문제 생성\n",
    "url = 'https://www.nytimes.com/2019/11/14/us/politics/trump-tax-returns-supreme-court.html'\n",
    "raw_txt_name = 'CSAT_34.txt'\n",
    "prepro_txt_name = 'prepro_34.txt'\n",
    "learned_txt_name = 'blank_34.txt'\n",
    "num_words = 500\n",
    "\n",
    "def blank_34(url, raw_txt_name, prepro_txt_name, learned_txt_name, num_words, textrank = False):\n",
    "    \n",
    "    # 크롤링을 통해 특정 기사 url에서 본문 및 기사 헤드라인을 추출하기 \n",
    "    text, title, blank_sentence = crawl_news(url, raw_txt_name, num_words) # url과 raw_txt_name 파일명 입력\n",
    "    # 추출한 데이터를 문장별 딕셔너리로 저장하기\n",
    "    test_dict = bf_create_blank(raw_txt_name, prepro_txt_name) # raw_txt_name은 현재 디렉터리 기준 경로임. \n",
    "    # 셰봇이 학습시키기 (자연어 생성, 텍스트 요약)\n",
    "    shake_bot_blank(learned_txt_name, prepro_txt_name) # 셰봇이 학습시키기 (자연어 요약)\n",
    "    \n",
    "    # Textrank를 적용한 Rouge Score에 따라 선지 생성 \n",
    "    if textrank == True:\n",
    "    \n",
    "        question_list, sort_rouge_list = for_rouge_test2(learned_txt_name, test_dict, blank_sentence)\n",
    "    \n",
    "    # Textrank를 적용하지 않은 Rouge Score에 따라 선지 생성\n",
    "    elif textrank == False:\n",
    "        \n",
    "        question_list, sort_rouge_list = for_rouge_test(learned_txt_name, test_dict)\n",
    "    \n",
    "    # 생성된 선지 (학습의 결과물)을 그대로 사용하지 않고, unk 및 유사어 사전을 활용해서 paraphrasing\n",
    "    for idx, question in enumerate(question_list):\n",
    "        \n",
    "        replace_sentence = delete_unk(prepro_txt_name, question)\n",
    "            \n",
    "            # ans_sent = replace_sentence # 동사 빈칸 문제 생성을 위해 정답 선지 사전 대입 \n",
    "            \n",
    "        # 정답 선지를 제외한 선지에 대해 전처리 적용 (이 부분도 재검토)\n",
    "        if idx > 0:\n",
    "            \n",
    "            token = word_tokenize(replace_sentence)\n",
    "            \n",
    "            tag = pos_tag(token)\n",
    "            \n",
    "            # 동사, 명사, 형용사에서 유사어로 치환\n",
    "            v_list = [t[0] for t in tag if (t[1] == \"VB\") or (t[1] == \"VBD\") or (t[1] == \"VBG\") or (t[1] == \"VBG\") or (t[1] == \"VBN\") or (t[1] == \"VBP\") or (t[1] == \"VBZ\") or\n",
    "                     (t[1] == \"NN\") or (t[1] == \"NNS\") or (t[1] == \"NNP\") or (t[1] == \"NNPS\") or (t[1] == \"JJ\") or (t[1] == \"JJR\") or (t[1] == \"JJS\") or (t[1] == \"RB\")]\n",
    "            \n",
    "            # 유사어 치환\n",
    "            for v in v_list:\n",
    "                \n",
    "                paraphrasing_dict = sym_vocab.crawling_dict(v)\n",
    "                \n",
    "                paraphrasing_list = paraphrasing_dict[v]\n",
    "                \n",
    "                # 유사어가 존재하는 경우 랜덤하게 1가지 추출\n",
    "                if paraphrasing_list == list:\n",
    "                \n",
    "                    random.shuffle(paraphrasing_list)\n",
    "             \n",
    "                    token[token.index(v)]= paraphrasing_list[0]\n",
    "                \n",
    "                # 유사어가 존재하지 않는 경우 최초 입력값 그대로 다시 반환\n",
    "                elif paraphrasing_list != list:\n",
    "                    \n",
    "                    token[token.index(v)] = paraphrasing_list\n",
    "                    \n",
    "            replace_sentence = ' '.join(token)\n",
    "        \n",
    "        # 빈칸 선지 1개, 지엽적 선지 4개 저장\n",
    "        question_list[idx] = replace_sentence\n",
    "    \n",
    "    for i, q in enumerate(question_list): # 선지 뽑아내기 \n",
    "        \n",
    "        token_q = word_tokenize(q)\n",
    "    \n",
    "        tag_q = pos_tag(token_q)\n",
    "        \n",
    "        # print(tag_q)\n",
    "    \n",
    "        for ix, t in enumerate(tag_q):\n",
    "            # 동사 유무 판단, 동사 위치를 인덱스 값으로 저장\n",
    "            if (t[1] == \"NN\") or (t[1] == \"NNS\") or (t[1] == \"NNP\") or (t[1] == \"NNPS\"):\n",
    "                \n",
    "                idx_num = ix\n",
    "                \n",
    "                break\n",
    "            \n",
    "        # 해당 문장에 동사가 이상 없이 존재하여, 문장에서 최초의 동사를 찾는다면\n",
    "        if ix == len(tag_q) - 1:\n",
    "            # 선지 리스트에 넣기    \n",
    "            question_list[i] = q \n",
    "            \n",
    "            break\n",
    "                \n",
    "        # 첫번째 원소의 경우 빈칸 정답으로 활용. \n",
    "        elif i == 0:\n",
    "            \n",
    "            save_sent = token_q[:idx_num + 1] # 동사 이전 문장까지 살리기\n",
    "            \n",
    "            save_sent = ' '.join(save_sent)\n",
    "            \n",
    "            save_sent = save_sent.capitalize()\n",
    "            \n",
    "            abandon_sent = token_q[idx_num + 1:] # 동사 이후 문장 선지화 \n",
    "            \n",
    "            abandon_sent = ' '.join(abandon_sent) \n",
    "            \n",
    "            question_list[i] = abandon_sent # 동사 이후 문장은 선지에 넣기 \n",
    "            \n",
    "        # 나머지 4개 원소의 경우 선지로 활용\n",
    "        else:\n",
    "            \n",
    "            abandon_sent = token_q[idx_num + 1:] # 동사 이후 문장 선지화 \n",
    "            \n",
    "            abandon_sent = ' '.join(abandon_sent) \n",
    "        \n",
    "            question_list[i] = abandon_sent # 동사 이후 문장은 선지에 넣기 \n",
    "             \n",
    "    # 빈칸으로 설정한 인덱스 추적 -> 해당 문장 번호를 빈칸으로 대체 \n",
    "    text = make_blank_verb(sort_rouge_list[0][0], text, blank_sent = save_sent)\n",
    "        \n",
    "    text = re.sub('\\n', ' ', text) # raw data에서 불필요한 줄바꿈표 제거 (상대적일듯)\n",
    "        \n",
    "    # 정답 빈칸이 현재 무조건 1번이므로, 랜덤하게 섞어주기 \n",
    "    random.shuffle(question_list)\n",
    "    \n",
    "    number_list = ['①', '②', '③', '④', '⑤']\n",
    "    \n",
    "    for ix, final_question in enumerate(question_list):\n",
    "        \n",
    "        final_question = number_list[ix] + ' ' + final_question\n",
    "        \n",
    "        question_list[ix] = final_question\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" 실제 문제 만들기 \"\"\"\n",
    "    \n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "    print('34. 다음 빈칸에 들어갈 말로 가장 적절한 것을 고르시오.')\n",
    "    print('-----------------------------------')\n",
    "    print(text)\n",
    "    print('\\n')\n",
    "    for question in question_list:\n",
    "        print(question)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    blank_34(url, raw_txt_name, prepro_txt_name, learned_txt_name,num_words, textrank = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
